%\documentclass[10pt,a4paper,oneside]{article}
% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode
\documentclass[10pt,article,oneside]{memoir} 
\usepackage{pdfsync}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
%\usepackage[ngerman]{babel} 

\usepackage{lipsum}
\usepackage{multicol}

\usepackage{amsmath, amssymb, amsthm, mathrsfs,nicefrac,MnSymbol}
\usepackage{multirow,tabularx,colortbl,hhline}
\usepackage{nicefrac}


\usepackage{listings}
\usepackage{color}
\usepackage{footnote}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{%frame=tb,https://www.overleaf.com/project/608bc77c801b16bbadb2210a
  language=sh,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\RequirePackage{etex}

\usepackage{graphicx,xcolor}
\graphicspath{{./figures/}}



% GRAPHICS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\usepackage{framed}
\definecolor{shadecolor}{rgb}{0.9,0.9,0.9}
\usepackage{watermark}
% see later for watermark declaration


% Theorem environments %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newtheorem{thm}{Theorem}[]
\newtheorem*{thm*}{Theorem}
\newtheorem{cor}{Corollary}[]
\newtheorem{lem}[]{Lemma}
\newtheorem{prop}[]{Proposition}
\newtheorem{protocol}[]{Protocol}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem*{defn*}{Definition}

\theoremstyle{remark}
\newtheorem{rem}[thm]{Remark}
\newtheorem{rems}[thm]{Remarks}
\newtheorem{rem*}[]{Remark}

%\renewcommand{\theenumi}{\roman{enumi}}
\renewcommand{\labelenumi}{(\theenumi)}
\newcommand\itemref[1]{(\ref{#1})}
%\newcommand{\COMent}[1]{}


% OWN FLOAT ENVIRONMENT %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\usepackage{newfloat, caption}
%\DeclareFloatingEnvironment[fileext=frm,placement={!ht},name=Protocol]{myfloat}
%
%\captionsetup[myfloat]{labelfont=bf}
%\usepackage[framemethod=TikZ]{mdframed}
%
%\newenvironment{framedprotocol}
%{
%	\begin{myfloat}
%	\begin{mdframed}%[roundcorner=10pt,backgroundcolor=blue!10]
%}
%{
%	\end{mdframed}
%	\end{myfloat}
%}


% PSEUDOCODE and MATH %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{algorithm,algorithmicx}
\usepackage{algpseudocode}

\usepackage[n,advantage,operators,sets,adversary,landau,probability,notions,logic,ff,mm,primitives,events, complexity,asymptotics,keys]{cryptocode}



\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\DeclareMathOperator{\N}{\mathbb{N}}
\renewcommand{\PP}{\mathbf{P}}
\newcommand{\OO}{\mathcal{O}}


\DeclareMathOperator{\param}{\mathsf{Par}}
\DeclareMathOperator{\gen}{\mathsf{Gen}}
\DeclareMathOperator{\setup}{\mathsf{Setup}}
\DeclareMathOperator{\indexer}{\mathsf{Index}}
\DeclareMathOperator{\comm}{\mathsf{Com}}
\DeclareMathOperator{\open}{\mathsf{Open}}
\DeclareMathOperator{\prove}{\mathsf{Prove}}
\DeclareMathOperator{\extract}{\mathsf{Extract}}
\DeclareMathOperator{\simulate}{\mathsf{Sim}}

\renewcommand{\adv}{\mathsf{Adv}}

%\renewcommand{\negl}{\nu}


%\newcommand{\trans}{\mathsf{Trans}}

%\DeclareMathOperator{\generate}{\mathsf{Generate}}
%\DeclareMathOperator{\present}{\mathsf{Present}}
%\DeclareMathOperator{\issue}{\mathsf{Issue}}
%\newcommand{\user}{\mathsf{User}}

% NOTES %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Select what to do with todonotes: 
% \usepackage[disable]{todonotes} % notes not showed
\usepackage[draft,textsize=small]{todonotes}   % notes showed




% HYPERREFS und URLs %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[bookmarks, colorlinks=false, pdftitle={The Latus PCD scheme based on Marlin}, pdfauthor={author}]{hyperref}
  \usepackage{url}

\author{
    Hab{\"o}ck, Ulrich\\
    \texttt{ulrich@horizenlabs.io}
    \and
    Garoffolo, Alberto\\
    \texttt{alberto@horizenlabs.io}
    \and
    Di Benedetto, Daniele\\
    \texttt{daniele@horizenlabs.io}
}

\begin{document}
%\frontmatter
\title{%
Darlin: Recursive proofs using Marlin
}
\date{
September 3, 2021
}
\maketitle




\begin{abstract}
This document\footnotemark describes \textit{Darlin}, a succinct zero-knowledge argument of knowledge based on the  Marlin SNARK \cite{Marlin} and the `dlog' polynomial commitment scheme from \cite{BootleGroth, Bulletproofs}.
Darlin addresses recursive proofs by integrating the amortization technique from Halo \cite{Halo} for the non-succinct parts of the dlog verifier, and we adapt their strategy for bivariate circuit encoding polynomials to aggregate Marlin's inner sumchecks across the nodes the recursive scheme.
We estimate the performance impact of inner sumcheck aggregation by about 25\% in a tree-like scheme of in-degree $2$, and beyond when applied to linear recursion.
% In this document we describe the \textit{Darlin} 
% proof carrying data scheme for the distributed computation of block and epoch proofs in a Latus sidechain of Zendoo \cite{Zendoo}.
% Recursion as well as base proofs rest on Marlin \cite{Marlin} using the Pasta curves \cite{PastaCurves} and the `dlog' polynomial commitment scheme from \cite{BootleGroth, Bulletproofs}.
% We apply the amortization technique from Halo \cite{Halo} to the non-succinct parts of the verifier, and we adapt their strategy 
% for bivariate circuit encoding polynomials to aggregate Marlin's inner sumchecks across the nodes of the proof carrying data scheme.
% Regarding performance, the advantage of Darlin over a scheme without inner sumcheck aggregation is about 30\% in a tree-like scenario as ours, and beyond when applied to linear recursion.
\end{abstract}

\footnotetext{This paper is the full version of the previously published extended abstract `Darlin: A proof-carrying data scheme based on Marlin'.}
%Keywords: SNARKs, recursive proofs, aggregation scheme

\begin{KeepFromToc}
  \tableofcontents
\end{KeepFromToc}

%\mainmatter
\chapter{Introduction}
\label{s:Introduction}

Succinct non-interactive arguments of knowledge (SNARKs) are the basis for secure decentralized computations, allowing to verify the correctness of a large number of operations by a single succinct and easy to verify cryptographic proof. 
Since the advent of SNARKs \cite{Groth:2010, Gennaro:2013} practical proof systems followed soon after, e.g. Pinocchio \cite{Pinocchio}, Groth16 \cite{Groth:2016}, and Groth17 \cite{Groth:2017}.  
Whereas the first SNARKs are intrinsically connected to pairings via non-standard knowledge commitments, 
%(and as such rely on trusted setup ceremonies),   
proof systems from the second wave, such as Sonic \cite{Sonic}, Aurora \cite{Aurora}, Marlin \cite{Marlin}, or Plonk \cite{Plonk}, are built in a modular way on any polynomial commitment scheme.

%and allow to choose the polynomial commitment scheme to be used within. 
 
To scale over large amounts of data to be processed, recursive arguments or more generally \textit{proof-carrying data (PCD) schemes} \cite{PCD, SNARKsfromPCD}, are inevitable. 
Recursive arguments verify the existence of a previous such, and their performance is largely dependent on how efficient the verifier itself is translated into an argument. 
The issue of such a translation step is that typically the assertion to be proven is arithmetized (as a relation or circuit) over a field of a different characteristic than the proof itself, and simulating the arithmetics of a `foreign' field is costly.
The most common approach to tackle the problem is using a $2$-cycle of elliptic curves \cite{AmicablePairs, Cycles}.
Such cycles are pairs of elliptic curves in which the subgroup of one curve is of the same prime order as the base field of the other. 
%For this reason many practical systems follow the approach of \cite{BCTV} using 2-cycles of elliptic curves \cite{AmicablePairs, Cycles}, in which the subgroup of one curve is of the same prime order as the base field of the other. %
Applied to pairing-based SNARKs the cycle approach requires high field sizes. 
The only known cycles are based on MNT curves of low embedding degree \cite{Cycles}, and as such they demand field sizes beyond $1,000$ bit to meet a reasonable level of security  \cite{GuillevicTNFS}\footnotemark. 
\footnotetext{
\cite{Coda} uses a cycle of MNT4/MNT6 curves with $753$ bit field sizes targeting a security level of $128$ bit.
However, improvements on the towered number field sieve \cite{GuillevicTNFS, GuillevicMNT} enforce to increase the field size up to $1,000$ bits.
}%
%The first recursive systems in practice \cite{BCTV, Coda} use such MNT cycles and hence rely on 
Second wave SNARKs are not necessarily bound to pairings, hence allow to use cycles of non pairing-friendly curves \cite{Halo, Halo2}, or such in which at least one of the curves is not pairing-friendly \cite{Mina}.  
Although allowing for smaller field sizes, the use of non pairing-friendly cycles introduces another issue.
Due to a lack of better alternatives, such constructions apply (a variant of) the `dlog' polynomial commitment scheme from \cite{BootleGroth} the verifier of which is linear in the size of the circuit to be proven; a serious obstacle for efficient recursion.
In their seminal work \cite{Halo}, Bowe et al. showed how to overcome the problem of linear verifier size by a novel approach called \textit{nested amortization}. 
%However, this performance bottleneck can be tackled by the principle of \textit{nested amortization} from Bowe, Griggs and Hopwood \cite{Halo}.
In nested amortization the proof system aggregates the computational `hard parts' of the verifier outside the circuit, reducing the verification of all of them to a single expensive check at the recursion end.

Since \cite{Halo} amortization schemes became an active field of research.
B\"unz et al. \cite{Buenz} gave a more modular approach to the \cite{Halo} concept of amortization (named accumulation scheme therein).
However, their approach is less performant than the one in \cite{Halo}, which directly integrates the amortization rounds into the argument system.
Boneh et al. \cite{HaloInfinite} extend the concept of amortization to \textit{private} aggregation schemes for polynomial commitments, which allow to aggregate entire opening proofs along the nodes of a PCD scheme.
An even more radical approach for rank one constraint systems (R1CS) is followed by  \cite{PrivateAggregationR1CS}, who aggregate entire R1CS solutions over recursion.
Although both approaches lead to a significant speed-up of recursion, they come at the cost of increased proof sizes for the PCD.
The private witnesses aggregated across the nodes are as large as the circuit itself. 
For a Marlin verifier it is at about $1 \text{ MiB}$ at minimum, and multiples of that in typical applications \cite{PrivateAggregationR1CS}.


\medskip
In this document we describe the \textit{Darlin} proof carrying data scheme, the recursive SNARK for a Latus sidechain of Zendoo \cite{Zendoo}, a blockchain system which supports cross-chain communication. 
Latus sidechains are highly customizable blockchains which share the same token as the Zendoo mainchain they are bootstrapped from, and the Darlin scheme is used to provide succinct proofs of correct side chain state transitions.
%\todo{A bit more about sidechains?}. 
%The Darlin PCD scheme rests on an ecosystem of independent (\textit{SNARK provers})
%In this scheme tree-like recursion is applied whenever timing is critical (e.g. when computing block proofs) and otherwise sequentially.
Darlin is based on the Marlin argument system, modified in order to handle the aggregation of both Marlin's inner sumchecks and the `dlog' hard parts. 
%We apply tree-like recursion whenever timing is critical (e.g. when computing block proofs) and otherwise sequentially.
%For our implementation we choose the Pasta cycle \cite{PastaCurves} of non pairing-friendly curves, and use the optimization techniques from \cite{Halo} to  reduce the size of the verifier in circuit. %we use Halo's endomorphism-based scalar multiplication and defer non-native field operations to the `next' field in recursion.
According to our estimates, we expect the advantage of Darlin over standard Marlin (without inner sumcheck aggregation) to be about $30\%$ when `merging' two previous proofs, at the cost of only tripling the proof size, cf. Table \ref{t:DarlinVsMarlin}. 

%\footnotetext{
%The Darlin timings given in Table \ref{t:DarlinVsMarlin} are extrapolated from benchmarks which simulate the computational effort of a Darlin prover.
%}

\begin{table}
\caption{
The impact of inner sumcheck amortization: 
Comparison of Marlin/dlog \cite{Marlin} versus Darlin for a PCD node which verifies two previous proofs, both using the Pasta curves \cite{PastaCurves}, and using the optimization techniques from \cite{Halo} to  reduce the size of the verifier in circuit. 
We ``segmentize'' dlog commitments to speed up the prover.
See Section \ref{s:Performance} for details.
As our implementation is not ready yet, the prover times are \textit{estimates} for an Amazon EC2 G4dn instance (4 Intel Xeon@2.5 GHz + 1 NVIDIA T4). 
}
\label{t:DarlinVsMarlin}
\begin{center}
\hspace*{-0.3cm}
\begin{tabular}{|l|c|c|c|c|}
%\multicolumn{2}{l}{Domain size $|H|$} & \multicolumn{3}{c|}{$2^{19}$}
%\\
\hline
\multicolumn{2}{|l|}{dlog segment size$^\dag$} &  $2^{19}$ & $2^{18}$ &$2^{17}$
\\\hline
% L=1  Darlin  $94 k$ & $102 k$ & $123 k$
% L=1 Marlin $121 k$ & $153 k$ & $224 k$
\multirow{2}{*}{constraints}  & Marlin$^{*}$ &  $\approx 320$ k & $\approx  384$ k & $\approx 520$ k
\\
& Darlin &   $\approx 290$ k & $\approx 320$ k & $\approx 390$ k
\\\hline
\multirow{2}{*}{proof size}  &  Marlin$^{*}$ & $\approx 4.2$ kB & $\approx 4.6$ kB & $\approx 5.3$ kB
\\
 & Darlin &  $\approx 15.3$ kB & $\approx 15.7$ kB &  $\approx 16.8$ kB
\\\hline
\multirow{2}{*}{prover time}  &  Marlin$^{*}$ &  $\approx 16.5$ s & $\approx 15.9$ s & $\approx 15.38$ s
\\
 & Darlin &   $\approx 12.3$ s ($9.6$ s$^{**}$) & $\approx 11.7$ s ($9.1$ s$^{**}$) & $\approx 11.4 $  ($8.8$ s$^{**}$)
\\\hline
% \multicolumn{5}{r}{$^{\dag}${\small We ``segmentize'' dlog commitments to speed up the prover, see Section \ref{s:Performance}. 
%, we use ``segmentation'' of the dlog commitment scheme 
%}}
%\\
\multicolumn{5}{r}{$^{*}${\small Assuming R1CS density $d=2$, which is large enough in our applications.
}}
\\
\multicolumn{5}{r}{$^{**}${\small only at the two lowest levels of a proof tree, where aggregation is trivial.}}
\end{tabular}
\end{center}
\end{table}


The document is organized as follows. 
In Section \ref{s:CoboundarySumcheck} we describe a variant of the univariate sumcheck argument from \cite{Aurora, Marlin}, inspired by the grand product argument of Plonk \cite{Plonk}. 
This variant does not rely on degree bound proofs and allows a more lightweight zero-knowledge randomization. 
In Section \ref{s:Marlin} we informally describe our variant of Marlin, which besides using a slightly different matrix arithmetization applies the sumcheck argument from Section \ref{s:CoboundarySumcheck}.
In Section \ref{s:Amortization} we recapitulate the amortization strategy for the dlog hard parts, explain the aggregation of Marlin's inner sumcheck across multiple circuits, and summarize the main recursive argument.
All formal definitions and proofs are postponed to the appendix, Section \ref{s:Appendix}.
%Finally, we conclude  with a description of the Darlin proof carrying data scheme in Section \ref{s:PCD} and give a short description of our implementation strategy in Section \ref{s:Implementation}. 

 



%In the conclusio we relate to other 
%which allows to parallelizing the effort of each tree level amongst a set of `SNARK provers'



%\chapter{Preliminaries}
%\label{s:Preliminaries}
%
%definition R1CS  and QAP
%witness relation
%\[
%\mathcal R =\big\{ (x,w)\in F^\ell\times F^{m-\ell} : \vec y =(x,w) \text{ satisfies }  (A\cdot\vec y) \odot (B\cdot \vec y) = C\cdot \vec y \big\}
%\]

\chapter{Preliminary notes}

%The largest part of this document is kept informal, while a rigorous treatment is postponed to the full version.
%However, even then some notions need to be settled at the beginning, at least to such an extent so that a subsequent exposure does not lack clarity.

%\medskip
% Assume that $\mathcal R$ is a polynomial decidable binary relation, consisting of pairs $(x,w)$, where we call $x$ statement and $w$ witness.
% Informally, an \textit{interactive argument} for $\mathcal R$ is a protocol between a computationally bounded prover and a verifier with the aim to prove that for a given statement $x$ there  exists a $w$ such that $(x,w)\in\mathcal R$. 
% Typically, the argument has a setup procedure which provides a common reference string supporting instances $(x,w)$ up to a certain maximum size.
% Beside that common reference string, the verifier is given only $x$ while the prover has $w$, and after a certain number of steps the verifier is convinced of the fact that $x\in\mathcal R$ and accepts, otherwise she rejects.
% The notions of completeness, zero-knowledge and knowledge soundness are given in Appendix \ref{s:ArgumentSystems}.
% If the transcript of the conversation (and hence the number of steps) is sub-linear in the size of the instance $(x,w)$, then the argument is called \textit{succinct}.
 
% A \textit{polynomial commitment scheme} is a commitment scheme for polynomials over some finite field which additionally comes with an interactive argument for proving the value of a committed polynomial at a queried point. 
% This \textit{evaluation argument} (or, \textit{opening proof}) is a succinct interactive argument for statements of the form $(C,x,y)$, where $C$ is the commitment of a witness polynomial $p(X)$ for which $y=p(x)$.
% See Appendix \ref{s:PolynomialCommitments} for the formal definitions.

Whenever appropriate, we formulate our protocols as \textit{algebraic oracle proofs}, with oracles as an information-theoretic model for homomorphic polynomial commitments.
%algebraic variant of interactive oracle proofs \cite{IOPs}.
An algebraic oracle proof is a multi-round protocol in which the prover responds to verifier challenges with oracles for some \textit{low-degree} polynomials, receives another challenge from the verifier. 
The prover replies with some other oracles, and so on.
The verifier is allowed to query these oracles for the values of any linear combination of their polynomials at any point she chooses.
As in algebraic holographic proofs \cite{Marlin}, the verifier may access some of the inputs only via oracle queries, but we do not assume that these oracles stem from a (circuit-specific) setup phase.
Algebraic oracle proofs can be viewed as a variant of fully linear interactive oracle protocols \cite{FullyLinearPCPs}, considering an evaluation query as a linear functional of the polynomial to be queried.
%Our input oracles contain private or public polynomials, typically created at running time before calling the protocol.
However, we shall not dwell on their separate information-theoretic security properties, nor we explicitly provide the compiler which transforms these into their corresponding ones for the resulting interactive argument systems when instantiating the oracles by a polynomial commitment scheme.
Instead, the proof of our main recursive argument from Section \ref{s:FullProtocol} relies on a compiler as used implicitly in the security analysis of the batch evaluation protocol from \cite{HaloInfinite}.




\chapter{A cohomological sumcheck argument}
\label{s:CoboundarySumcheck}

Let $F$ be a finite field, $H$ be a multiplicative subgroup of order $n$, and assume that $p(X)$ is a polynomial of arbitrary degree.
The univariate \textit{sumcheck argument} from \cite{Aurora, Marlin} is an algebraic oracle proof for showing that
\[
\sum_{x\in H} p(x) = 0.
\]
The sumcheck argument is the key ingredient to Marlin's way of proving a witness polynomial satisfying the rules of a given circuit (see Section \ref{s:Marlin}).
It is based on the fact that the above sum is equal to $n$ times the constant term of the polynomial, if $p(X)$ is of \textit{reduced form}, i.e. of a degree strictly less than the domain size $|H|=n$.
Hence showing that the reduced form of $p(X)$ has constant term zero, i.e.
\begin{equation}
\label{e:Sumcheck}
p(X) = X\cdot g(X) + h(X) \cdot (X^n-1),
\end{equation}
for some polynomials $h(X)$ and $g(X)$ whereas $deg(g(X)) < n-1$, proves the claimed sum.
To convince the verifier of \eqref{e:Sumcheck} the prover provides the oracles for $p(X)$ $g(X)$ and $h(X)$, which we denote by 
\[
[p(X)], [g(X)], [h(X)],
\]
together with a proof that $deg(g(X))\leq n-1$.
In response the verifier samples a random challenge $z\sample F$ on which the oracles are queried for $p(z)$, $g(z)$, $h(z)$.
These evaluations are used to validate the identity \eqref{e:Sumcheck} at $X=z$. 
In order to obtain (honest verifier) zero-knowledge, the prover samples a random `mask' polynomial $s(X)$ of degree at least $n$ and proves that
\begin{equation}
\label{e:SumcheckZk}
\hat p(X) = p(X) + s(X)
\end{equation}
sums up to $\sigma = \sum_{z\in H} s(z)$, which  is done by an ordinary sumcheck argument for $\hat p(X) - \sigma/n$.
%\[
%\hat p(X) =\frac{\sigma}{n} +  X\cdot g(X) + h(X)\cdot (X^n-1),
%\]
See \cite{Marlin} for the details.
%\footnote{We remark that in \cite{Marlin} the mask polynomial $s(X)$ is chosen of same degree as $p(X)$ 
%However, a careful inspection similar to that for Lemma \ref{l:zk} below,  shows that degree $n-1 + b$, where  is sufficient to ensure bounded zero-knowledge up to $b$ queries.}.

%\medskip
% Plonk \cite{Plonk} uses an alternative approach for proving a `grand product' $\prod_{z\in H}p(z) = 1$ in their permutation argument. 
% For that, the prover provides the oracle for a polynomial $Z(X)$ which satisfies
% \begin{equation}
% \label{e:coboundaryPlonk}
% Z(g\cdot X)/Z(X) = p(X) \mod (X^n -1),
% \end{equation}
% where $g$ is a generator of the multiplicative subgroup $H$. 
% %We point out that Plonk's argument can be seen as cohomological one, and carry it over to the additive sumcheck.
% %We observe that this is a cohomological argument:
% Given the action of $\Z$ on $H$ defined by the `shift' transformation $T(x) = g\cdot x$, consider the cumulative products
% \[
% f_p(k, x) = \prod_{i=0}^{k-1} p(T^i(x)),
% \]
% for $x\in H$ and $k\in\Z$.
% The function $f_p$ is a \textit{cocycle} with values in the multiplicative group $F^*$ of the field, which means that  $f_p(m+k,x) = f_p(m,T^k(x))\cdot f_p(k,x)$ for every $m,k\in \Z$ and $x\in H$.
% In terms of cocycles, identity \eqref{e:coboundaryPlonk} states that $f_p$ is a \textit{coboundary}, which means that there exists a polynomial $Z(X)$ such that
% \[
% f_p(k,x) = Z(T^k(x))\cdot Z(x)^{-1},
% \]
% for every $x\in H$, $k\in \Z$. 
% This polynomial is called the \textit{boundary} of the coboundary $f_p$.
% It is a well-known fact that a cocycle is a coboundary if and only if the products over the closed loops of the action on $H$ vanish, that is $\prod_{z\in H} p(z) = 1$, as $g$ is a generator of $H$. 
%$ \prod_{i=0}^{n-1} p(T^i (x)) = 1$.
%(We remark that the additional condition $Z(1)=1$ used in \cite{Plonk} is not needed.)
%We observe that the grand product argument from Plonk \cite{Plonk} is a `cohomological' one and hence is easily adapted to our additive setting.
%We moreover show that zero-knowledge is obtained by a much more lightweight randomization than above.
%
%The argument is as follows.
%Instead using $g(X)$, the prover shows that $p(X)$
%is a \textit{coboundary} with respect to the group action on $H$ by providing the \textit{boundary function} $U(X)\in F[X]/(X^n-1)$ which is characterized by the \textit{coboundary identity}
%\begin{equation}
%\label{e:sumcheckBoundary}
%U(g\cdot X) - U(X) =  p(X) \mod (X^n-1).
%\end{equation}
%Here $g$ is a generator of $H = \{1, g, g^2,\ldots, g^{n-1}\}$.

\medskip
Our sumcheck argument carries over the grand product argument from Plonk \cite{Plonk} to the additive setting.
Instead of using the reduced form of the polynomial $p(X)$ in question, the prover shows that the additive \textit{cocycle} with respect to the group action of $\Z$ on $H$ defined by $g$,
\begin{equation}
f_p(k,X) = \sum_{i=0}^{k-1} p(g^i\cdot X)
\end{equation}  
is a \textit{coboundary}, which is characterized by the following folklore Lemma.
\begin{lem}
\label{lem:coboundary}
Let $H$ be a multiplicative subgroup of a finite field $F$ and let $g$ be a generator of $H$.
For any univariate polynomial $p(X)$ of arbitrary degree we have $\sum_{z\in H} p(z) = 0$ if and only if there exists a polynomial $U(X)$ such that 
\begin{equation}
\label{e:sumcheckBoundary}
U(g\cdot X) - U(X) =  p(X) \mod (X^n-1).
\end{equation}
\end{lem}

%We postpone the proof of Lemma \ref{lem:coboundary} to Appendix \ref{s:CoboundaryLemma}.
\begin{proof}%[Proof of Lemma \ref{lem:coboundary}]
Suppose that $\sum_{z\in H} p(z) = 0$. 
Define $U(X)$ on $H$ by intitializing $U(g^0) = U(1)$ to any arbitrary value, and setting
\[
U(g^k) = U(1) +\sum_{i=0}^{k-1} p(g^i)
\]
for $k=1,\ldots, n-1$.
By definition $U(g^{k+1}) = U(g^k) + p(g^k)$ for all $k$, $0\leq k\leq n-2$.
The equation also holds for $k=n-1$, since the full cycle sum $\sum_{i=0}^{n-1}p(g^i) = \sum_{z\in H} p(z) $ vanishes. 
This shows that $U(g\cdot z)-U(z) = p(z)$ for all $z$ in $H$, thus any extension $U(X)$ beyond $H$ satisfies the claimed identity $U(g\cdot X)-U(X)=p(X) \bmod (X^n-1)$.
The other direction of the proof is obvious.
\end{proof}

\medskip
The main advantage of the coboundary approach is that the algebraic oracle proof for equation \eqref{e:sumcheckBoundary} allows a more lightweight zero-knowledge randomization than that of equation \eqref{e:Sumcheck}:
Since no reduced form is needed for $U(X)$,  we can simply randomize $U(X)$ by means of the vanishing polynomial of $H$, 
\begin{align}
\label{e:sumcheckRand2}
\hat U(X) &= U(X) + (c_0 + c_1\cdot X) \cdot (X^n-1),
\end{align}
with uniformly random $c_0$, $c_1\sample F$, assuming that $\hat U(X)$ is not queried beyond the sumcheck protocol.
We describe the sumcheck argument as an algebraic oracle proof for polynomials from $R= F[X]/(X^n-1)$ with the aim to prove that the prover knows an element from  $R$ which is subject to the sumcheck $\sum_{x\in H} p(X) = 0$.


%The oracles $[\hat p_i(X)]$ are given as inputs to the verifier, and it is assumed that each of them is queried at most another $b_i$ times beyond the sumcheck protocol, respectively.

\begin{protocol}[Coboundary sumcheck]
\label{p:coboundarySumcheck}
Let $H$ be a multiplicative subgroup of a finite field $F$, $g$ be a generator of $H$ having order $n$.
The prover is given $p(X)$ from $R= F[X]/(X^n-1)$ subject to $\sum_{x\in H}p(x) = 0$, and the verifier is given the oracle of a random representant $\hat p(X)= p(X) + r(X)\cdot (X^n-1)$, where $r(X)$ is sampled uniformly from the set of polynomials of degree strictly less than $b+1$.
\footnote{The bound $b\geq 0$ corresponds to the maximum number of allowed queries for $[\hat p(X)]$ beyond the sumcheck protocol.}
%The verifier is given the oracles for $\hat p_i(X)$ and wants to be convinced upon the fact that $\sum_{x\in H} p(x) = 0$.
\begin{enumerate}
\item 
The prover $P$ computes $U(X)$ of $deg(U(X))<n$ according to the coboundary identity \eqref{e:sumcheckBoundary}.
It computes $\hat U(X)$ as in \eqref{e:sumcheckRand2},  with $c_0,c_1\sample F$, and the quotient polynomial $h(X)$
% of degree $deg(h(X)) \leq \max(d-n,1)$ 
satisfying
\begin{equation}
\label{e:coboundaryZk}
\hat U(g\cdot X)- \hat U(X) = \hat p(X) + h(X)\cdot (X^n-1).
\end{equation}
$P$ then sends $[\hat U(X)]$, $[h(X)]$ to the verifier.
\item
The verifier $V$ samples a random challenge $z\sample F\setminus H$ and queries the oracles  $[\hat U(X)]$, $[h(X)]$, and $[\hat p(X)]$ for their values at $z$. 
(The oracle aborts, if $z\in H$.)  
$V$  uses these values to verify identity \eqref{e:coboundaryZk} at $X=z$, and accepts if valid. 
(Otherwise, the verifier rejects.)
\end{enumerate}
\end{protocol}

% The security notions \textit{(computational) soundness} and \textit{(perfect) honest verifier zero-knowledge} for algebraic oracle proofs are given in Section \ref{s:IOPs}. 

% \begin{thm} 
% Protocol \ref{p:coboundarySumcheck} is a computationally sound algebraic oracle proof for the sumcheck relation $\mathcal R=\{ (H\leq F^*, p(X)\in F^{\leq N(\lambda)}[X]) :  \sum_{x\in H} p(x) = 0 \}$ having soundness error $\varepsilon=\nicefrac{N(\lambda)}{|F|-|H|}$ and satisfying perfect honest verifier zero-knowledge.
% \end{thm}

% Using a computationally binding and perfectly hiding polynomial commitment scheme (for which the field size $|F|$ increases exponentially in the security parameter), the protocol is compiled into a computationally sound and perfect zero-knowledge argument.

The security analysis of Protocol \ref{p:coboundarySumcheck} (applied to a specific $\hat p(X)$) is given in the course of the proof of Theorem \ref{thm:CompleteProtocol}.
%, which discusses in detail the zero-knowledge variant of Coboundary Marlin from Section \ref{s:Marlin}.
As a separate algebraic oracle proof it is perfectly complete and computationally knowledge sound, assuming the size of $H$ is negligible compared to the size of the field $F$.  
It is succinct and perfectly honest verifier zero-knowledge, assuming that each the oracles $[\hat p(X)]$ is queried outside the protocol at most another $b$ times (and $[\hat U(X)]$ is not queried at all). 
The latter is an immediate consequence of the fact that the conditional distribution of 
\[
(v_1,v_2,v_3,v_4)= \big(\hat U(g\cdot z), \hat U(z), \hat p(z), h(z)\big),
\]
conditional to $z\notin H$, is uniform on the relation $\mathcal R_z = \{(v_1,v_2,v_3,v_4)\in F^4: v_1 -v_2 - v_3 = v_4\cdot (z^n-1)\}$.
%As in the proof of Theorem \ref{thm:CompleteProtocol}, 
If one instantiates the oracle with a computationally binding (Definition \ref{def:Binding}) and perfectly hiding (Definition \ref{def:Hiding}) polynomial commitment scheme, the opening proof of which is an argument of knowledge (Definition \ref{def:ArgumentOfKnowledge}), then the protocol is compiled into a succinct honest verifier zero-knowledge argument of knowledge.
%(The compiler is the same as used for the proof of Theorem \ref{thm:CompleteProtocol}.)



%Note that the coboundary equation \eqref{e:sumcheckBoundary} determines $Z(X)$ only up to an additive constant, but any solution $Z(X)$ is sufficient to prove that $f(X)-\frac{a_0}{n}$ sums up to zero.
%The boundary $Z(X)$ is computed in $n$ field multiplications given the coefficients of $f(X)$.% (or in another $O(n\cdot\log(n))$ mutliplications from its Lagrange representation).
%This means that in terms of arithmetics on the witnesses the prover has to do $n$ multiplications more compared to the Fourier sumcheck, but doesn't need to prove any degree bound on $Z(X)$.


\chapter{Coboundary Marlin}
\label{s:Marlin}

%Marlin \cite{Marlin} is preprocessing zk-SNARK for rank one constraint systems which is based on an \textit{interactive oracle protocol (IOP)} between a \textit{prover} and a \textit{verifier}. 

This section describes \textit{Coboundary Marlin}, a slight variant of the Marlin SNARK \cite{Marlin}.
We introduce two changes:
First, we replace Marlin's  sumcheck argument by the coboundary argument from Section \ref{s:CoboundarySumcheck}. 
Second, we\footnote{We would like to thank A. Querol for pointing out that \cite{Lunar} also choose the Lagrange kernel. 
As a consequence, our version of the lincheck is exactly the same as theirs.} make use of the Lagrange kernel
\begin{equation}
\label{e:LagrangeKernel}
L_n(X,Y) = \frac{1}{n}\cdot \frac{Y \cdot (X^n-1) - X\cdot (Y^n-1)}{X - Y}
\end{equation}
%from Section \ref{s:Preliminaries} 
instead of the non-normalized version $R(X,Y)=\frac{X^n - Y^n}{X-Y}$. 
%in both of Marlin's sumchecks.
%\footnotetext{As Marlin uses Lagrange encoding we consider the Lagrange kernel as the natural choice.}
The Lagrange kernel shares the same key properties as $R(X,Y)$.
It can be evaluated \textit{succinctly}, and allows a \textit{practical} sumcheck representation for the bivariate circuit polynomials, as shown below.
However, we  point out that our favor for the Lagrange kernel is mainly for esthetic reasons.
Using it allows us to argue directly with the  bivariate circuit polynomials instead of a derivative in both of Marlin's sumcheck arguments as well as our aggregation strategy from Section \ref{s:Amortization}.
% that comment is put to the section "The protocol";
%\todo{Note that formal analysis of coboundary Marlin is given in an updated version of the document.}


\section{Arithmetization}
\medskip
%Assume that $A$, $B$, $C$ are the rank matrices of
%Given an arithmetic circuit over $F$, we write is encoded as rank-one constraint system (R1CS)
We assume an arithmetic circuit $\mathcal C$ over $F$ being represented by a \textit{rank-one constraint system (R1CS)}, i.e.
\begin{equation}
\label{e:R1CS}
(A\cdot y)\odot(B\cdot y) = C\cdot y,    
\end{equation}
where we assume that $A$, $B$, $C$ are $n\times n$ matrices over $F$, $\cdot$ is the vector matrix product and $\odot$ denotes the entry-wise (Hadamard) product of vectors.
The witness vector $y\in F^n$ is composed of a public part $x$ and a private part $w$, i.e.  $y=(x\|w)$.
%\todo{explain vector matrix prod.}
Notice assuming quadratic matrices is no loss in generality, as the constraint system may always be padded with dummy constraints or variables. 
Moreover, we presume that $|F|-1$ is divisible by a high power of two, assuring the existence of sufficiently large multiplicative subgroups of 2-adic order.
Subgroups of such smooth order allow for a fast Fourier transform which runs in time $O(n\log(n))$, where $n$ is the order of the subgroup.
(In the sequel we call such subgroups \textit{FFT domains}.)

In Marlin the R1CS equations are expressed over the FFT domain $H=\{z\in F : z^n-1=0\}$ using Lagrange encoding.
That is, given an arbitrary enumeration $\{z_1,\ldots,z_n\}$ of $H$ a vector $y=(y_k)_{k=1}^n$ is associated with the polynomial 
\begin{equation}
\label{e:LagrangeRepresentation}
    y(X) =  \sum_k y_k \cdot L(X, z_k).
\end{equation}
In other words, $(y_k)$ is the vector of coordinates with respect to the Lagrange basis $(L(X,x_k))_k$. 
Therefore $y\in F^n$ is a solution of \eqref{e:R1CS} if and only if its associated polynomial $y(X) =  \sum_k y_k \cdot L(X, z_k)$ satisfies
\begin{align}
\label{e:QAPc}
y_A(X)\cdot y_B(X) & = \sum_{z\in H}  C(X,z)\cdot y(z) \mod (X^n-1),
\\
\intertext{where}
\label{e:QAPa}
y_A(X) &= \sum_{z\in H}  A(X,z)\cdot y(z) \mod (X^n-1),
\\
\label{e:QAPb}
y_B(X)  &= \sum_{z\in H}  B(X,z)\cdot y(z)  \mod (X^n-1).
\end{align}
In these equations,  $A(X,Y)$, $B(X,Y)$, $C(X,Y)$ are the bivariate polynomials with the entries of the R1CS matrices $A$, $B$, $C$ respectively as Lagrange coordinates, 
\begin{equation}
\label{e:BivariateMatrixRepresentation}
M(X,Y) =  \sum_{i,j=1}^n M_{i,j} \cdot L(X, z_i)\cdot L(Y, z_j),
\end{equation}
for $M=A,B,C$.
The double sum in \eqref{e:BivariateMatrixRepresentation} is made amenable to a univariate sumcheck argument by indexing its non-zero terms over yet another FFT domain  $K=\{ w\in F : w^m - 1 = 0\}$, again assuming the existence of a sufficiently large smooth multiplicative subgroup. 
%In practice the matrices $M=A, B, C$ are sparse, and we observed that $K$ is about $2$ times larger than $H$ for the circuits we consider, see Section \ref{s:NoteOnPerformance}.
As in Marlin, we denote by
\begin{equation}
\label{e:valANDrowANDcol}
val_M(X), row_M(X), col_M(X)\in F[X]/(X^m-1)
\end{equation} 
the polynomials of degree $<m$ which index $M$'s non-zero values, their row and column indices (the latter two regarded as points from $H$, as in \eqref{e:BivariateMatrixRepresentation}), so that   
\begin{align*}
M(X, Y) &= \sum_{w \in K } val_M(w) \cdot L(X , row_M(w)) \cdot L(Y, col_M(w)).
\end{align*}
%As the values of $row_M(w)$ and $col_M(w)$ are in $H$, both $row_M(w)^n-1$ and $col_M(w)^n-1$ are zero, and hence
Since   $L_n(X, z)= \frac{1}{n}\cdot \frac{z \cdot (X^n-1)}{X - z}$ whenever $z$ is from $H$, we have
\begin{multline}
\label{e:MatrixRepresentationSumcheck}
M(X, Y)
=\frac{(X^n-1)\cdot (Y^n-1)}{n^2}
\\
\cdot\sum_{w \in K } 
\frac{val_M(w) \cdot row_M(w)\cdot col_M(w)}{\left(X - row_M(w)\right)\cdot\left( Y - col_M(w)\right)} \mod (X^m-1).
\end{multline}
This representation, which differs slightly from \cite{Marlin}, is the one we use for the second sumcheck argument, the `inner sumcheck'.
We assume that for $M=A,B,C$, the precomputed polynomials
\begin{align}
\label{e:rowcol}
row.col_M(X) &= row_M(X)\cdot col_M(X) \mod (X^m-1),
\\
\label{e:valrowcol}
val.row.col_M(X)&= val_M(X)\cdot row_M(X)\cdot col_M(X) \mod (X^m-1),
\end{align}
regarded of degree $<m$, are also part of the verifier key.




\section{The protocol}

In Marlin, public circuit inputs $x=(x_i)$ define the Lagrange representation of the \textit{input polynomial} 
\[
x(X) = \sum_{z\in I} x_i \cdot L_\ell(z, X) \in F[X]/(X^\ell - 1)
\]
over an properly sized input domain $I\leq H$ of size $\ell$, and the full circuit state polynomial $y(X)$ is combined via
\begin{equation}
y(X) =  x(X) + (X^\ell - 1 )\cdot w(X),
\end{equation}
using a gauged witness polynomial $w(X)\in F^{< n - \ell}[X]$.
The prover provides the oracles for the private witness polynomial $w(X)$, $y_A(X)$, $y_B(X)$ and convinces the verifier of the R1CS identities \eqref{e:QAPc}, \eqref{e:QAPa}, and \eqref{e:QAPb}.
These three identities are reduced to a single one by building a random linear combination based on a challenge $\eta\sample F$, i.e.
\begin{equation}
\label{e:lincheck}
y_\eta(X) = \sum_{z\in H}  T_\eta(X,z) \cdot y(z)  \mod (X^n-1),
\end{equation}
with
\begin{equation*}
y_\eta(X)  = y_A(X) + \eta \cdot y_B(X) + \eta^2\cdot y_A(X)\cdot y_B(X),
\end{equation*}
and
\begin{equation*}
T_\eta (X,Y) =A(X,Y)+ \eta\cdot B(X,Y)+ \eta^2\cdot C(X,Y).
\end{equation*}
%(We notice that using the powers of $\eta$ slightly differs from choosing arbitrary random scalars $\eta_A$, $\eta_B$, $\eta_C$ as in \cite{Marlin}.)
The linear identity \eqref{e:lincheck} is reduced to a sumcheck over $H$ by sampling a polynomial $R(X,\alpha)$ using a suitable kernel $R(X,Y)$, $\alpha\sample F$, and applying it via scalar product to both sides of the equation.
This yields
\begin{equation*}
\sum_{z\in H}  \langle R(X,\alpha),T_\eta(X,z)\rangle_H \cdot y(z) 
=\langle R(X,\alpha), y_\eta(X)\rangle_H,
\end{equation*}
%where $\alpha$ is randomly sampled from outside $H$.
hence
\begin{equation}
\label{e:OuterSumcheckR}
\sum_{z\in H}  \langle R(X,\alpha),T_\eta(X,z)\rangle_H \cdot y(z) - R(z,\alpha)\cdot y_\eta(z) = 0.
\end{equation}
%or $\sum_{z\in H}  \tilde T_\eta(\alpha ,z) \cdot y(z) -R(z,\alpha), y_\eta(z) = 0$, where $\tilde T_\eta(\alpha, z) =  \langle R(X,
%\alpha),T_\eta(X,z)\rangle_H$.
%uses
Choosing the Lagrange kernel $L_n(X,Y)$ for $R(X,Y)$, $\langle L_n(X,\alpha), T_\eta(X,z)\rangle_H = T_\eta(\alpha,z)$, since $T_\eta(X,z)$ is of degree less than $n$ (see Appendix \ref{s:LagrangeKernel}). 
Hence equation \eqref{e:OuterSumcheckR} is equal to
\begin{equation}
\label{e:OuterSumcheck}
\sum_{z\in H}  T_\eta(\alpha,z) \cdot y(z) -  L_n(z,\alpha)\cdot  y_\eta(z) = 0.
\end{equation}
Equation \eqref{e:OuterSumcheck} is the central identity to be proven by the protocol.

% If one uses the generalized derivative $R(X,Y) = u_H(X,Y)= (Z_H(X)-Z_H(Y)) / (X-Y)$ then the outer sumcheck equation is 
% \begin{equation}
% \label{e:OuterSumcheck}
% \sum_{z\in H}  T_\eta^*(\alpha,z) \cdot y(z) -  u_H(z,\alpha)\cdot  y_\eta(z) = 0,
% \end{equation}
% where $M^*(X,Y)$ is the gauged matrix polynomial of individual degree 
% \[
% M^*(X,Y) =  u_H(X,X)\cdot M(X,Y) \mod (X^n-1).
% \]
% See Section \ref{s:LagrangeKernel}.

\medskip
We describe the protocol as algebraic oracle proof.
As for every oracle proof the announcement of all queried values are postponed to the end of it.
%See \cite{DarlinFullProtocol} for a detailed treatment.

\subsubsection{Initialization}
In the first step the prover computes the polynomials\footnote{%
Unless stated otherwise we assume polynomials $p(X)$ from $F[X]/(X^n-1)$ of reduced form, i.e. of degree $<n$.
} 
$w(X)$,  $y_A(X)$, $y_B(X)\in F[X]/(X^n - 1)$ from their Lagrange representations, and chooses random representants 
\[
\hat w(X), \hat y_A(X), \hat y_B(X) \in F^{<n+1}[X] 
\]
according to the sampling rule $\hat q(X) \sample q(X) + F \cdot (X^n-1)$ using randomizer polynomials of degree zero.
It sends their oracles $[\hat w(X)]$, $[\hat y_A(X)]$, $[\hat y_B(X)]$ to the verifier,  who returns the randomnesses $\eta\sample F$ and $\alpha\sample F\setminus H$ for Equation \eqref{e:OuterSumcheck}.

\subsubsection{Outer sumcheck}
To prove equation \eqref{e:OuterSumcheck} we apply the coboundary argument from Section \ref{s:CoboundarySumcheck} to $\hat p(X) := T_\eta(\alpha, X)\cdot \hat y(X) - L_n(X,\alpha) \cdot \hat y_\eta(X)$, where 
\begin{align*}
\hat y(Y) &:= x(Y) + (Y^\ell-1)\cdot \hat w(Y), 
\\
\hat y_\eta(Y) &:=  \hat y_A(Y)+\eta\cdot \hat y_B(Y) + \eta^2\cdot \hat y_A(Y)\cdot \hat y_B(Y).
\end{align*}
The prover computes the boundary polynomial $U_1(X) \in F[X]/(X^n - 1)$, chooses a random representant 
\[
\hat U_1(X)\in  F^{<n+2}[Y]
\]
of it, and computes $h_1(X)\in F^{< 2\cdot n}[X]$ for the outer sumcheck identity
%% For zk we have
%%      deg \hat y_\eta <= n + n,
%%      deg L_n(X,\alpha) = n - 1,
%% and hence
%%      deg h_1(X) <= n - 1 + 2n - deg (X^n -1) = 2n - 1.
%% For non-zk we have
%%      deg \hat y_\eta <= 2(n-1),
%%      deg L_n(X,\alpha) = n - 1,
%% and hence
%%      deg h_1(X) <= n - 1 + 2(n - 1)  - deg (X^n - 1) = 2n - 3.
\begin{multline}
\label{e:OuterSumcheckCoboundary}
T_\eta(\alpha,X) \cdot \hat y(X) -  L_n(X,\alpha)\cdot \hat y_\eta(X) 
\\
= \hat U_1(gX) - \hat U_1(X) + h_1(X)\cdot (X^n-1),
\end{multline}
where $g$ is a generator of $H$.
It then sends $[\hat U_1(X)]$, $[h_1(X)]$ together with $[T_\eta(\alpha, X)]$ to the verifier.
The verifier samples another random challenge $\beta\sample F\setminus H$ and queries the oracles for
$\hat w(\beta), \hat y_A(\beta), \hat y_B(\beta), 
T_\eta(\alpha,\beta), \hat U_1(g\cdot \beta)$, $\hat U_1(\beta), h_1(\beta)$,
which are used for checking the identity \eqref{e:OuterSumcheckCoboundary} at $X=\beta$.

%A valid outer sumcheck proves that $y(X)$ is a solution of the QAP for \textit{some} matrices but doesn't tell which one, unless we relate the value allegedly representing $T_\eta(\alpha,\beta)$ to the circuit in question.
%Note that the Lagrange kernel can be efficiently evaluated by the verifier, using the succinct representation $L(X, Y)=\frac{1}{n}\cdot \frac{Y \cdot v_H(X)- X\cdot v_H(Y)}{X - Y}$.

% If one uses the generalized derivative $u_H(X,Y)$ as $R(X,Y)$, then the outer sumcheck identity is
% \begin{multline*}
% T^*_\eta(\alpha,X) \cdot \hat y(X) -  u_H(X,\alpha)\cdot \hat y_\eta(X) 
% \\
% = \hat U_1(gX) - \hat U_1(X) + h_1(X)\cdot (X^n-1),
% \end{multline*}


\subsubsection{Inner sumcheck}
To  prove that  $v$ as provided by the oracle\footnote{
There is room for improvement:
One can omit the oracle for the public polynomial $T_\eta(\alpha,X)$ and use the uniquely determined value from \eqref{e:OuterSumcheckCoboundary} at $X=\beta$, which is $v= \frac{L_n(\beta,\alpha)\cdot \hat y_\eta(\beta) +  \hat U_1(g\cdot \beta) - \hat U_1(\beta) + h_1(\beta)\cdot (\beta^n-1)}{\hat y(\beta)}$.
This reduces the overall commitment effort as given in Table \ref{t:CoboundaryMarlin} by another multi-scalar multiplication of size $n$.
}
for $T_\eta(\alpha,X)$ in fact stems from the circuit polynomials $M(X,Y)$, $M=A,B,C$ we adapt Marlin's inner sumcheck to our representation \eqref{e:MatrixRepresentationSumcheck}.
Using these we obtain
\begin{align}
\label{e:innerSumcheck}
T_\eta(\alpha, \beta)
%=\frac{(\alpha^n-1)\cdot (\beta^n-1)}{n^2}\cdot 
=\sum_{w \in K }\sum_{M=A,B,C} \eta_M  \cdot \frac{val.row.col_M(w)}{(\alpha - row_M(w)) \cdot (\beta - col_M(w))},
\end{align}
where $(\eta_A,\eta_B,\eta_C)= \frac{(1-\alpha^n)\cdot (1-\beta^n)}{n^2}\cdot (1,\eta,\eta^2)$.
We apply the coboundary sumcheck to 
\[
p(X) = \sum_{M=A,B,C} \eta_M  \cdot \frac{val.row.col_M(X)}{(\alpha - row_M(X)) \cdot (\beta - col_M(X))},
\]
regarded as a reduced element from $F[X]/(X^m - 1)$.
%in a slightly modified manner. 
The prover computes $U_2(X)$ from $F[X]/(X^m-1)$ satisfying
\begin{align*}
p(X) = \frac{T_\eta(\alpha,\beta)}{m} + U_2(g_K X) - U_2(X)  \mod (X^m-1),
\end{align*}
and then multiplies both sides with the denominator 
\begin{align*}
b(X) &= \prod_{M=A,B,C} (\alpha -row_M(X))\cdot (\beta - col_M(X))
\\
&=\prod_{M=A,B,C} \left(\alpha\beta - \beta \cdot row_M(X) - \alpha \cdot col_M(X) + row.col_M(X)\right),
\end{align*}
where $row.col_M(X)$ are the precomputed products \eqref{e:rowcol} from the prover key. 
This yields the \textit{inner sumcheck} identity
\begin{multline}
\label{e:InnerSumcheck}
\sum_{M=A,B,C} \eta_M \cdot val.row.col_M(X) \cdot b_M(X)
\\
= b(X) \cdot \left(\frac{v}{m} + U_2(g_K X) - U_2(X)\right) + h_2(X)\cdot (X^m -1 ),
\end{multline}
where 
\[
b_M(X) = \prod_{N \neq M} \left(\alpha\beta - \beta \cdot row_N(X) - \alpha \cdot col_N(X) + row.col_N(X)\right),
\]
$g_K$ is a generator of $K$, and $h_2(X) \in F^{< 3\cdot m - 3}[X]$.
The prover sends the oracles $[U_2(X)]$ and $[h_2(X)]$ to the verifier, who samples a random challenge $\gamma\sample F$, 
on which the oracles are queried for
$row_M(\gamma), col_M(\gamma), row.col_M(\gamma)$, $val.row.col_M(\gamma)$, where $M=A,B,C$,
and $U_2(g_K \cdot\gamma), U_2(\gamma), h_2(\gamma)$.
These values, together with $v$, are used by the verifier to check the identity \eqref{e:InnerSumcheck} at $X=\gamma$.


\subsection{Security}

The security analysis of Coboundary Marlin is similar to that of our main recursive argument, Theorem \ref{thm:CompleteProtocol}.
As for Theorem \ref{thm:CompleteProtocol}, we stress the fact that we use the Halevi-Micali \cite{PoKHaleviMikali} notion of proof of knowledge with negligible knowledge error.
The proof can be found in the appendix, Section \ref{s:ProofCoboundaryMarlin}.
\begin{thm}
\label{thm:CoboundaryMarlin}
Instantiating the oracle by a computationally binding and perfectly hiding polynomial commitment scheme (Definition \ref{def:Hiding} and \ref{def:Binding}), Coboundary Marlin is a succinct, perfect honest verifier zero-knowledge (Defintion \ref{def:ZeroKnowledge}) argument of knowledge (Definition \ref{def:ArgumentOfKnowledge}) for the R1CS relation 
\[
\mathcal R =\big\{ ((A,B,C,x),w) : y=(x,w) \text{ satisfies } (A\cdot y)\odot(B\cdot y) = C\cdot y \big\}.
\]
\end{thm}

Using the Fiat-Shamir transform the interactive argument is transformed into a zk-SNARK with analog security properties   
in the random oracle model.

\section{A note on performance}
\label{s:NoteOnPerformance}
Marlin's outer sumcheck takes place over the FFT domain $H$, the size of which covers the number of constraints/variables of the  constraint system.
In practice circuits yield about the same number of variables as constraints, hence it is reasonable to take $n$ the number of constraints as measure for the computational effort of the outer sumcheck, assuming a sufficiently smooth order of $F^*$ to optimally match $n$.
The inner sumcheck runs over the FFT domain $K$ of size $m\approx \max_{M=A,B,C}\|M\|$ ($\|M\|$ is the number of non-zero entries in $M$), again under the assumption of sufficient smoothness.
This domain is by the factor
\[
d = \frac{\max_{M=A,B,C}\|M\|}{n}
\]
larger, where $d$ is the \textit{R1CS density} of the circuit. 
%We shall call $d$ the \textit{R1CS density} of the constraint system.
The R1CS density is the average number of variables per constraint.
In practice, we observed values between $d=1.5$ and $d=2$ for the circuits we target. 
(These circuits implement elliptic curve arithmetics over non-extension fields and the $x^5$-Poseidon hash \cite{Poseidon} with an internal state of $3$ field elements.)

\begin{table}[h!]
\caption{%
Computational effort of the (coboundary) zk-Marlin prover, using an elliptic curve based linear polynomial commitment scheme. 
We only count fast Fourier transforms $\textsf{FFT}(a)$ in terms of their domain size $a$, and elliptic curve multi scalar multiplications $\textsf{MSM}(b)$ in terms of the number of scalars $b$. 
(Without opening proof.)
}
\label{t:CoboundaryMarlin}
\vspace*{3mm}
\centering
\hspace*{-0.5cm}
\begin{tabular}{|l|c|c|}
\cline{2-3}
\multicolumn{1}{c|}{} &polynomial arithm. & commit
\\\hline
 intial round  &  $3 ~\textsf{FFT}(n)$   & $3~\textsf{MSM}(n)$
\\
%
% outer sumcheck: T_\eta(\alpha,Y)\cdot y(Y) - L(\alpha,Y)\cdot y_\eta(X) - U_1(gX) + U_1(X)
%
% 	- T_\eta(\alpha,Y) over multiplication domain 2|H|:	1 FFT(n) + 1 coset-FFT(n) (+ vector matrix products + domain eval of L(X,alpha))
%	- y(Y) over multiplication domain 2|H|:				1 coset-FFT(n) (w(X) is computed in the initial round)
% 	- product of the two							1 FFT(2n)
% 	- y\eta(X) over mult. domain 3 |H|				2 ( 2 coset-FFT(n)) (y_A(X) and y_B(X) are computed in the initial round)
%	- product of y_eta with L(\alpha,Y)				1 FFT(3n)
%	- U_1(X),										1 FFT(n)
% 												--------------------------------------
%	overall										2 FFT(n) + 6 coset-FFT(n) + 1 FFT(2n)+ 1 FFT(3n),
% 	or taking full domain eval instead of coset FFT		2 FFT(n) + 2 FFT(2n) + 3 FFT(3n) 
%													~ 15 FFT(n)
\multirow{2}{*}{%
outer sumcheck}  
	&  $2 ~\textsf{FFT}(n) + 2~\textsf{FFT}(2n)$% 
		& \multirow{2}{*}{$2 ~\textsf{MSM}(n) + 1~\textsf{MSM}(2n)$}
\\ & $+ 3~\textsf{FFT}(3n)$ &
%\\
%&& --  & $2 \cdot\max(n,m)$ 
\\
%
% inner sumcheck:   sum_M \eta_M * val.row.col_M(X) - b(X)*(value/m + U_2(gX) - U_2(X)),
% 	with b(X) = Prod_M (alpha*beta + beta* row_M(X)+ alpha*col_M(X) + row.col_M(X))
%
%	- computing U_2(X)								1 FFT(m)
%	- row_M(X), col_M(X), row.col_M(X) eval over 4*|K|		precomputed, otherwise 1 FFT(4m)
%	- product b(X)*(value/m+U_2(gX)-U_2(X))				1 FFT(4m)
%													----------------
% 	overall											1 FFT(m) + 1 FFT(4m) ~ 5 d FFT(m)
%	without precomp									1 FFT(m) + 2 FFT(4m) ~ 9 d FFT(m)
inner sumcheck  
	& $1~\textsf{FFT}(m) + 1~\textsf{FFT}(4m) $
		& $1~\textsf{MSM}(m) +1~\textsf{MSM}(3m)$ 

\\\hline
overall 
	 &  $\approx  (18 + 5\cdot d)~\textsf{FFT}(n)$ 
		& $\approx (7+4\cdot d)~\textsf{MSM}(n)$\footnotemark
\\\hline
\end{tabular}
\end{table}
\footnotetext{%
By omitting the oracle for $T_\eta(\alpha,X)$, as pointed out in the previous footnote, the overall commitment effort is reduced to $(6 + 4\cdot d)~\textsf{MSM(n)}$ 
}



\chapter{Recursion}
\label{s:Amortization}

Our recursive scheme is based on Coboundary Marlin and the \cite{Buenz} variant of the \textit{dlog polynomial commitment scheme} from \cite{BootleGroth}.
We take Coboundary Marlin without inner sumcheck as succinct argument, and we aggregate both the non-succinct parts of the opening proof verifier, as well as the correctness checks usually served by the inner sumchecks, which is verifying that the commitment intended for 
\[
T_\eta(\alpha,Y) = \sum_{M=A,B,C} \eta_M\cdot M (\alpha, Y)
\]
in fact carries these polynomials.
%
Aggregation of the non-succinct part of the dlog verifier (the \textit{dlog hard parts}) relies on the same principle as introduced by Halo \cite{Halo}.
% and \cite{Buenz}.
% , viewing the committer key of the final round of the inner product argument as commitment to the polynomial
% \begin{equation*}
% h(\vec\xi, X) = \prod_{i=0}^{k-1} ( 1- \xi_{k-1-i} \cdot X^{2^i}),
% \end{equation*}
% where $k$ is the number of rounds of the argument, and $\vec\xi=(\xi_0,\ldots, \xi_{k-1})$ their reduction challenges.
The way we aggregate the inner sumchecks is a generalization of Halo's strategy for their circuit encoding polynomial $s(X,Y)$, and we  extend it across circuits to serve a reasonable number of instances $\mathcal C_i=\{A_i,B_i,C_i\}$ simultaneously.
As a separate `stand-alone' protocol, our strategy may be taken as \textit{public aggregation scheme} in the sense of \cite{HaloInfinite}, or an \textit{(atomic) accumulation scheme} according to \cite{Buenz, PrivateAggregationR1CS}.
%
However, for efficiency reasons we choose Halo's `interleaved' approach instead of the blackbox constructions from  \cite{Buenz, PrivateAggregationR1CS, HaloInfinite}, and let the rounds of both the argument system and the aggregation scheme share the same opening proof.

In our recursive argument certain previous proof elements  $(acc_i)_{i=1}^\ell$ called \textit{accumulators} are `passed' through inputs of the `current' circuit and post-processed within the run of the current argument.
%We assume that it is known which part of the circuit inputs $x$ is dedicated  
Formally, $(acc_i)_{i=1}^\ell$ satisfy a given predicate $\phi$, 
\[
\phi(acc_i)=1, \quad i=1,\ldots,\ell,
\]  
and are mapped to dedicated inputs of the current circuit.
Beyond the rounds for proving satisfiability of the current circuit, the accumulators $(acc_i)_{i=1}^\ell$ are aggregated within some extra rounds into a new instance, the `current' accumulator $acc$, which is again subject to $\phi(acc)=1$.
Altogether our recursive argument is of the form
\[
\big\langle \prove ((acc_i)_{i=1}^\ell, (x,w), pk), \verify((acc_i)_{i=1}^\ell,x,vk) \big\rangle,
\]
where $(x,w)$ are public and private circuit witnesses, $pk$ and $vk$ are  the prover and verifier key for both Marlin and the aggregation scheme, and the new $acc$ is output to both prover and verifier.  
%(We assume that $(acc_i)_{i=1}^\ell$ is consistent with the circuit inputs $x$.)
%In this chapter we only informally sketch our aggregation strategies, a formal treatise will be given in the full version.



\section{Inner sumcheck aggregation} 
\label{s:InnerSumcheckAggregation}

Here, the accumulator consists of a commitment $C$ and the succinct description of the circuit polynomial $T_\eta(z,Y)$ intended to be represented by $C$, i.e. the point $z\in F$ and the randomnesses $\vec\eta=(\eta_A,\eta_B,\eta_C)\in F^3$, 
\[
acc_{T} = (z, \vec\eta, C).
\]
The corresponding predicate $\phi_T$ is satisfied if and only if $C$ is the commitment of $T_\eta(z,Y)$ (using commitment randomness zero).
The prover reduces the correctness of several accumulator instances to that of a single new one, and the verifier validates the correctness of this reduction while keeping track of the polynomial descriptions (i.e. the point $z$ and the coefficient vector $\vec\eta$)  by herself.
We sketch the strategy assuming a single previous accumulator.

There, a previous instance $(\alpha',\vec\eta',C')$ is `merged' with  $(\alpha,\vec\eta,C)$ of the current outer sumcheck.
In a first step, the prover reduces the  `multi-point', `multi-polynomial' instance\footnote{%
Here `multi-point' refers to the different points $\alpha$, $\alpha'$, and `multi-polynomial' to the different polynomials defined by $\vec\eta$, $\vec\eta'$.
}
$T_{\vec\eta'}(\alpha',Y)$, $T_{\vec\eta}(\alpha,Y)$ to a single-point, multi-polynomial instance 
\[
T_{\vec\eta'}(X,\beta), T_{\vec\eta}(X,\beta),
\] 
with random $\beta\sample F$, by providing the commitments to these new  polynomials and proving consistency via polynomial testing:
If the old polynomials evaluate at the challenge $\beta$ to the same values as the new polynomials at the old point, respectively, then correctness of the new polynomials overwhelmingly implies that of the old ones.
Using the same principle once again, correctness of the single-point multi-polynomial instance is then reduced in batch to a single-point single-polynomial instance 
\[
\lambda\cdot T_{\vec\eta'}(\alpha'',Y) + T_{\vec\eta}(\alpha'',Y) = T_{\lambda\cdot\vec\eta' + \vec\eta}(\alpha'',Y),
\] 
where $\lambda,\alpha''\sample F$ are random.
Note that the resulting polynomial is again of the form $T_{\vec\eta''}(\alpha'',Y)$ with $\eta''=\lambda\cdot\vec\eta' + \vec\eta$.
For the reduction, the prover shows that the linear combination $\lambda\cdot T_{\vec\eta'}(X,\beta) + T_{\vec\eta}(X,\beta)$ opens at the new challenge $X=\alpha''$ to the same value as the new polynomial  $\lambda\cdot T_{\vec\eta'}(\alpha'',Y) + T_{\vec\eta}(\alpha'',Y)$ at the old point $Y=\beta$.
Again, correctness of the new polynomial overwhelmingly implies correctness of the old ones.
%Overall, we have sketched that if $\phi_T$ is satisfied for both instances $(\alpha',\vec\eta',C')$ and $$

Protocol \ref{p:InnerSumcheckAggregation} is regarded as a subprotocol of our complete recursive argument Protocol \ref{p:CompleteArgument}, right after the outer sumcheck.
We formulate it as an algebraic oracle protocol, considering commitments as oracles. 
%For brevity we let the prover send oracles of polynomials to the verifier as in an algebraic oracle protocol, by which we mean that she sends their commitments.
%The opening proof of all evaluation claims is postponed to the end of the complete recursive argument.
%We further restrict ourselves to a single previous accumulator only, the general case is straight-forward.

\begin{protocol}[Inner sumcheck aggregation]
\label{p:InnerSumcheckAggregation}
Suppose that $acc_T'=(\alpha',\vec\eta',[T'(Y)])$ is a previous accumulator, intended to represent an oracle for $T'(Y)= T_{\vec\eta'}(\alpha',Y)$, and $(\alpha,\vec\eta, [T(Y)])$ is as provided by the prover in the current outer sumcheck, intended to represent an oracle for $T(Y)= T_{\vec\eta}(\alpha,Y)$, with $\vec\eta =(1,\eta,\eta^2)$.
Aggregation of $acc_T'$ and $(\alpha,\vec\eta,[T(Y)])$ is done according to the following steps immediately processed after the outer sumcheck.
\begin{enumerate}
\item
\label{p:InnerSumcheckAggregation:Step1}
Given $\beta$, the random challenge from the outer sumcheck, the prover sends the oracles for the `bridging polynomials'
\[
T_{\vec\eta}(X,\beta), T_{\vec\eta'}(X, \beta)\in F[X]/(X^n-1),
\]
on which the verifier responds with random $\lambda,\gamma\sample F$.
 %(Note that if $v_1=v_2$ and $v_1'=v_2'$, then correctness of $C$ and $C'$ is reduced to the correctness of $[T_{\vec\eta}(X,\beta)]$ and $[T_{\vec\eta'}(X, \beta)]$.)

\item
\label{p:InnerSumcheckAggregation:Step2}
Given $\lambda,\gamma$ from the verifier, the prover `responds' with the oracle for
\[
T''(Y) = T_{\vec\eta}(\gamma, Y) + \lambda \cdot  T_{\vec\eta'}(\gamma, Y).
\]
\end{enumerate}
The verifier queries $[T_{\vec\eta}(X,\beta)]$, $[T_{\vec\eta'}(X,\beta)]$ for their corresponding values $v_1$, $v_2$ at $X=\alpha$ and $\alpha'$, and checks them against the values of $[T(Y)]$, $[T'(Y)]$ at $Y=\beta$, respectively.
It also queries $[T''(Y)]$ at $Y=\beta$ and checks its value against that of  the linear combination $[T_{\vec\eta}(X,\beta)]+\lambda [T_{\vec\eta'}(X,\beta)]$ at $X=\gamma$.
If these checks succeed, then the verifier accepts and the new accumulator is 
\[
acc_T''=(\alpha'',\vec\eta'', C'') = (\gamma, \vec\eta + \lambda\cdot \vec\eta', [T'(Y)]).
\]
%with $\alpha''=\gamma$, $\eta''=\vec\eta + \lambda\cdot \vec\eta'$ and $C''=[T''(Y)]$ from above.
%correctness of this accumulator overwhelmingly implies the correctness of both the previous and the current instances.
\end{protocol}
%The cost of an accumulation step consists of $3$ multi-scalar multiplication of size $n$ (compared to an equivalent of $4\cdot d$ multi-scalar multiplication the same size in the inner sumcheck), and the computation of the three polynomials $t(X,z_2)$, $t'(X,z_2)$ and $t''(z_3,Y)$, which essentially cost 5 FFT on domain $H$ (compared a rough equivalent of $9\cdot d~\mathsf{FFT}(n)$). 

A formal analysis of Protocol \ref{p:InnerSumcheckAggregation} is given in the course of the security proof of the complete recursive argument.
As a stand-alone argument having its own opening proof, the protocol defines a \textit{(perfectly) complete} and \textit{sound accumulation scheme} for the predicate $\phi_T$ in the sense of \cite{Buenz}: %(The randomness $\beta$ has to be sampled at)
If both $acc_T'$ and $(\alpha,\eta,C)$ satisfy the predicate $\phi$, so does $acc_T''$.
And if $\phi(acc_T'')=1$, then with overwhelming probability both $\phi(acc_T')$ and $\phi(\alpha,\eta,C)=1$.
%A proof of these properties is given in the security discussion of the complete argument from Section \ref{s:FullProtocol}.


\section{Generalization to several circuits} 
\label{s:InnerSumcheckAggregationGeneral}

The aggregation strategy from Section \ref{s:InnerSumcheckAggregation} is easily extended to serve multiple circuits  $C_1,\ldots, \mathcal C_L$ simultaneously.
This `cross-circuit' generalization is especially useful in `non-homogeneous' chemes which are composed by a variety of recursive circuits.
%
Lets assume that the R1CS matrices $A_i,B_i,C_i$ of the circuits $\mathcal C_i$, $i=1,\ldots,L$, are padded to the same square dimension so that we may regard their
\[
A_i(X,Y), B_i(X,Y), C_i(X,Y),
\]
as bivariate polynomials over the same domain $H\times H$.
As in the single-circuit setting we leverage the linearity of the commitment scheme and keep track of a single \textit{cross-circuit polynomial}
\begin{equation}
\label{e:CrossCircuitT}
T_{H}(\alpha,Y)=\sum_{i=1}^L T_{i, \vec\eta_i}(z,Y)= \sum_{i=1}^L \sum_{M= A_i,B_i,C_i} \eta_{M,i}\cdot M(\alpha, Y)
\end{equation}
by means of the cross-circuit coefficient vector $H=(\vec\eta_1,\vec\eta_2, \ldots, \vec\eta_L)$.
The \textit{cross-circuit accumulator} for the collection $\mathcal C=\{\mathcal C_1,\ldots, C_L\}$ is of the form
\[
acc_\mathcal C= (\alpha, H, C),
\]
with $\alpha\in F$, coefficient vector $H=(\vec\eta_1,\ldots, \vec\eta_L)\in (F^{3})^L$, and an element $C$ from the commitment group.
The corresponding predicate $\phi_\mathcal C$ is satisfied if and only if $C$ is in fact the dlog commitment of $T_H(\alpha,Y)$, using blinding randomness zero.

%\medskip
%The changes to Protocol \ref{p:InnerSumcheckAggregation} are obvious.
%Assume that $acc_\mathcal C'=(\alpha',H',C')$ is the previous cross-circuit accumulator and that the current outer sumcheck processes $\mathcal C_k=\{A_k,B_k,C_k\}$.
%In step \eqref{p:InnerSumcheckAggregation:Step1} the prover provides the bridging polynomials for the current $T_{k,\vec\eta_k}(\alpha,Y)$ as well as the cross-circuit combination $T_{H'}(\alpha',Y)$,  
%\[
%T_{k,\vec\eta}(X,\beta), T_{H'}(X, \beta)\in F[X]/(X^n-1),
%\] 
%and in step \eqref{p:InnerSumcheckAggregation:Step2} he responds with 
%\[
%T_{H''}(\gamma, Y) = T_{k,\vec\eta_k}(\gamma, Y) + \lambda\cdot T_{H'}(\gamma,Y),
%\]
%which is of again a cross-circuit linear combination with 
%$
%H'' = \delta_k\cdot \vec\eta_k + \lambda\cdot H',
%$
%where $\delta_k\cdot\vec\eta_k$ is short for $(0,\ldots,0,\eta_k,0,\ldots,0)$, with $\eta_k$ at position $k$ of the $L$-tuple.

%Note that in the cross-circuit setting, both prover and verifier need to know the R1CS matrices of all the circuits in question.


\section{Accumulating the dlog hard parts}
\label{s:IPAAggregation}

The aggregation strategy for the non-succinct part of the dlog verifier is identical to that in \cite{Buenz}.
The opening proof for the dlog commitment is an inner product argument that uses the folding technique from \cite{BootleGroth} to gradually reduce the opening claim on the initial full-length polynomial to one of half the size, until ending up with the opening claim of a single coefficient polynomial. 
The final committer key $G_{f}$ of the opening proof is a single group element which is the result of a corresponding folding procedure on the full-length committer key of the dlog scheme.
It equals the commitment of the succinct \textit{reduction polynomial}
\begin{equation}
\label{e:BulletPolynomial}
h(\vec\xi, X) = \prod_{i=0}^{k-1} ( 1- \xi_{k-1-i} \cdot X^{2^i}),
\end{equation}
where $k=\log|H|=n$ is the number of reduction steps and $\vec\xi = (\xi_i)_{i=0}^{k-1}$ their challenges.
The \textit{dlog accumulator} is of the form
\[
acc_{dlog} = (\vec\xi, C),
\]
where $\vec\xi\in F^k$ and $C$ is from the commitment group, and the corresponding accumulator predicate $\phi_{dlog}$ is satisfied if and only if $C$ is the commitment of $h(\vec\xi, X)$, using blinding randomness zero.

As Protocol \ref{p:InnerSumcheckAggregation}, the aggregation strategy is regarded as a subprotocol of the complete recursive argument Protocol \ref{p:CompleteArgument}, and for efficiency reasons we reuse the challenge $\gamma$ from the inner sumcheck aggregation.
We again restrict to the case of a single previous accumulator.

\begin{protocol}[dlog hard parts aggregation]
\label{p:IPAAggregation}
Suppose that $acc_{dlog}'=(\vec\xi',[h'(X)])$ is a previous dlog accumulator, with $[h'(X)]$ representing an oracle for $h'(X)=h(\vec\xi',X)$.
The following step is part of the complete recursive argument and processed immediately after Protocol \ref{p:InnerSumcheckAggregation}:
\begin{enumerate}
\item
\label{p:InnerSumcheckAggregationStep1}
The verifier queries $[h'(X)]$ for its value $v'$ at  $X=\gamma$ from step \eqref{p:InnerSumcheckAggregation:Step2} of Protocol \ref{p:InnerSumcheckAggregation}. 
\end{enumerate}
If $v'=h(\vec\xi', \gamma)$ then the verifier accepts.
The new accumulator $acc_{dlog}''=(\vec\xi'',C'')$ is the one from the dlog opening proof at the end of the complete protocol.
\end{protocol}


%Separated from the complete recursive argument, and having its separate opening proof, Protocol \ref{p:IPAAggregation} is as in \cite{Buenz}.

\section{The main recursive argument}
\label{s:FullProtocol}

The complete recursive argument is a composition of Coboundary Marlin's outer sumcheck for the `current' circuit, choosing `zero-knowledge bound' $b=1$, the aggregation rounds from the cross-circuit variant of Protocol \ref{p:InnerSumcheckAggregation}, and Protocol \ref{p:IPAAggregation}.
As in Section \ref{s:InnerSumcheckAggregationGeneral} we assume that the bivariate circuit polynomials $A_i(X,Y)$, $B_i(X,Y)$, $C_i(X,Y)$ are over the same domain $H\times H$, where $|H|=n$.
The query phases of these subprotocols are gathered at the end of the protocol, which is then concluded by the batch evaluation argument from \cite{HaloInfinite}.
%This `batched' opening proof reduces the multi-point assertion to a single-point assertion which is then handled as usual, leveraging the linearity of the commitment scheme.
%For the sake of completeness we present this batched opening proof in Appendix \ref{s:MultiPointSinglePoint}.

% There are two points we skipped in our overview of Marlin. 
% First, the public circuit inputs $x=(x_i)$ define the Lagrange representation of the \textit{input polynomial} 
% \[
% x(X) = \sum_{i} x_i \cdot L_\ell(x_i, X) \in F[X]/(X^\ell - 1)
% \]
% over an properly sized input domain $I\leq H$ of size $\ell$, and the full circuit state polynomial $y(X)$ is combined via 
% \[
% y(X) =  x(X) + (X^\ell - 1 )\cdot w(X),
% \]
% using a gauged witness polynomial $w(X)\in F^{< n - \ell}[X]$.
% Second, for obtaining zero-knowledge the $w(X)$, $y_A(X)$, $y_B(X)$ are randomized as sketched in Section \ref{p:coboundarySumcheck} and hence the outer sumcheck identity is
% \begin{multline}
% \label{e:OuterSumcheckCoboundaryZK}
% T_\eta(\alpha,X) \cdot \hat y(X) -  L_n(X,\alpha)\cdot \hat y_\eta(X)
% \\
% = \hat U_1(g\cdot X) - \hat U_1(X) + h_1(X)\cdot (X^n -1),
% \end{multline}
% where $\hat U_1(X)$ is a random representant of $U_1(X)$ and $\hat y(X)$, $\hat y_\eta$ are defined as their non-zk variants but using the randomized polynomials  $\hat w(X)$, $\hat y_A(X)$ and $\hat y_B(X)$, i.e.
% \begin{align*}
% \hat y(Y) &:= x(Y) + v_I(Y)\cdot \hat w(Y), 
% \\
% y_\eta(Y) &:=  \hat y_A(Y)+\eta\cdot \hat y_B(Y) + \eta^2\cdot \hat y_A(Y)\cdot \hat y_B(Y).
% \end{align*}

We formulate the complete argument with oracles for polynomials replaced by their dlog commitments, while keeping with the same notation $[p(X)]$.
For simplicity, we again restrict to the case of a single previous accumulator. 
The general case is straight-forward.



\begin{protocol}[Complete recursive argument]
\label{p:CompleteArgument}
Given a composed accumulator $acc'=(acc_{\mathcal C}',acc_{dlog}')$, where $acc_\mathcal C=(\alpha', H',C_T')$ is a cross-circuit accumulator for the collection $\mathcal C=\{\mathcal C_1,\ldots,\mathcal C_L\}$ and $acc_{dlog}'=(\vec\xi, C')$ is a dlog accumulator.
The recursive argument for an instance $(x,w)$ of the `current' circuit $\mathcal C_k$ from $\mathcal C$ is composed by the following steps.
\begin{enumerate}
\item
\textit{Intitialization for $\mathcal C_k$}:
The prover computes the gauged witness polynomial $w(X)$, $z_A(X)$, and $z_B(X)$ from $F[X]/(X^n-1)$ and chooses random representants
\begin{align*}
\hat w(X), \hat z_A(X), \hat z_B(X) \in F^{<n+1}[X]
\end{align*}
as described in Section \ref{s:Marlin}.
It sends their dlog commitments $[\hat w(X)]$, $[\hat z_A(X)]$, and $[\hat z_B(X)]$ to the verifier, who responds with $\eta,\alpha\sample F$.

\item
\textit{Outer sumcheck for $\mathcal C_k$}:
The prover computes 
\[
T_{\vec\eta}(\alpha, X)= \eta_A\cdot A(\alpha,X) + \eta_B\cdot B(\alpha,X) + \eta_C\cdot C(\alpha,X) \in F[Y]/(X^n-1)
\]
of the current circuit, using $\vec\eta=(\eta_A,\eta_B,\eta_C)=(1,\eta,\eta^2)$, and
\[
\hat U_1(X)\in  F^{<n+2}[Y], h_1(X)\in F^{< 2\cdot n + 1}[X]
\]
subject to the outer sumcheck identity \eqref{e:OuterSumcheckCoboundary}.
It sends  $[T_{\vec\eta}(\alpha, X)]$, $[\hat U_1(X)]$, $[h_1(X)]$ to the verifier, who returns another random challenge $\beta\sample F$.

\item
\textit{Inner sumcheck aggregation, Step 1}:
The prover computes the `bridging' polynomials for  
\[
T_{\vec\eta}(X,\beta), T_{H'}(X,\beta)\in F[X]/(X^n-1),
\]
and sends $[T_{\vec\eta}(X,\beta)], [T_{H'}(X, \beta)]$ to the verifier, who answers with another random $\lambda,\gamma\sample F$.
\item
\textit{Inner sumcheck aggregation, Step 2}:
The prover computes the cross-circuit linear combination
\[
T_{H''}(\gamma,Y) = T_{\vec\eta}(\gamma, Y) + \lambda \cdot  T_{H'}(\gamma, Y)\in F[Y]/(Y^n-1),
\]
and $[T_{H''}(\gamma,Y)]$ to the verifier.
\end{enumerate}
After these steps, both prover and verifier engage in the batch evaluation argument from \cite{HaloInfinite} for the dlog commitment scheme, applied to the queries as listed below.
If the queried values pass the checks of the outer sumcheck, Protocol \ref{p:InnerSumcheckAggregation} and Protocol \ref{p:IPAAggregation}, and if $(acc_{\mathcal C}',acc_{dlog}')$ match with the public input $x$ of the circuit, then the verifier accepts.
%, and uses $x$ thogether with the above openings to verify the outer sumcheck identity \eqref{e:OuterSumcheckCoboundary} at $X=\beta$, the inner sumcheck aggregation consistency checks as in Protocol \ref{p:InnerSumcheckAggregation}, and the dlog aggregation checks as in Protocol \ref{p:IPAAggregation}.
The new accumulator is $acc'' =(acc_\mathcal C'', acc_{dlog}'')$ with\footnotemark 
\[
acc_\mathcal C''=(\gamma,H'', C'') = (\gamma,  \vec\eta\cdot\delta_k + \lambda \cdot H', [T_{H''}(\gamma, Y)]) ,
\] 
and $acc_{dlog}''=(\vec\xi,G_f)$ from the above batch evaluation proof. 
\footnotetext{%
Here, $\vec\eta\cdot\delta_k$ denotes the vector which is $\vec\eta$ at the position of the current circuit $\mathcal C_k$ in the cross-circuit accumulator, and zero elsewhere.
}
\end{protocol}

The multi-point queries to be proven by the batch evaluation argument are as follows.
\begin{itemize}
\item[-] 
$[\hat w(X)], [\hat z_A(X)], [\hat z_B(X)], [\hat U_1(X)], [h_1(X)]$, $[T_{\vec\eta}(\alpha, X)]$ at $\beta$, as 
well as $[\hat U_1(X)]$ at $g\cdot \beta$,
\item[-]
$[T_{\vec\eta}(X,\beta)]$ at $\alpha$, $[T_{H'}(X,\beta)]$ at $\alpha'$,
and $[T_{H''}(\gamma, Y)]$, $C_T'$ from $acc_\mathcal C'$ at  $\beta$,
\item[-]
$[T_{\vec\eta}(X,\beta)]+\lambda\cdot [T_{H'}(X,\beta)]$ at $\gamma$, and 
$C'$ from $acc_{dlog}'$ at $\gamma$.
\end{itemize}
For the sake of completeness we summarize the batch evaluation argument in Section \ref{s:MultiPointSinglePoint}.

\medskip
The following theorem states that the main recursive argument, i.e. Protocol \ref{p:CompleteArgument} extended by the predicate check on $acc''_{\mathcal C}$, is a zero-knowledge argument of knowledge.
We point out that we use the Halevi-Micali \cite{PoKHaleviMikali} notion of proof of knowledge for negligible soundness error, see Definition \ref{def:KnowledgeSoundness} and Defintion \ref{def:ArgumentOfKnowledge}.

\begin{thm}
\label{thm:CompleteProtocol}
If the dlog commitment scheme is computationally binding (Definition \ref{def:Binding}) then Protocol \ref{p:CompleteArgument}, extended by the predicate verification on the resulting inner sumcheck accumulator $acc_\mathcal C''$, is a perfectly honest verifier zero-knowledge (Definition \ref{def:ZeroKnowledge}) argument of knowledge (Definition \ref{def:ArgumentOfKnowledge}) for the relation 
\begin{multline*}
\mathcal R = \big\{((\mathcal C, acc_\mathcal C',acc_{dlog}', x),w) :  (x,w)\in R_{\mathcal C_k}
\wedge \phi(acc_{\mathcal C}')=1
\\
\wedge  \phi_{dlog}(acc_{dlog}')=1 
%\\
\wedge (acc_C',acc'_{dlog}) \text{ is consistent with }x
\big\},
\end{multline*}
where $\mathcal C=\{\mathcal C_1,\ldots, \mathcal C_L\}$ is a collection of rank-one constraint systems.
Here, $R_{\mathcal C_k}$ denotes the R1CS relation given by the circuit $\mathcal C_k$, and  $\phi$ and $\phi_{dlog}$ are as in Section \ref{s:InnerSumcheckAggregationGeneral} and Section \ref{s:IPAAggregation}
\end{thm}
The proof of Theorem \ref{thm:CompleteProtocol} is given in Section \ref{s:ProofCompleteArgument}.
In practice we use the Fiat-Shamir transform to turn Protocol \ref{p:CompleteArgument} into a non-interactive argument of knowledge which is zero-knowledge against arbitrary polynomial time adversaries.
%A security analysis in the random oracle model is postponed to \cite{DarlinFullProtocol}.
%A formal treatment of its security properties in the random oracle model is given in an updated version of the paper.
%follows from  that of \cite{BuenzMaller}, who fill a technical gap in previous work on inner product arguments. 
%We postpone the statement and its discussion to 

\section{A note on performance}
\label{s:Performance}

Inner sumcheck aggregation is particularly effective when the number of previous accumulators is low, as seen from the operations counts in Table \ref{t:InnerSumcheckAggregation}.
For a single previous accumulator ($\ell=1$) representing the case of linear recursion, the prover effort for the recursive argument is comparable to that of standard Marlin for a circuit of R1CS density $d=1$.
Having $\ell=4$ previous accumulators, as in our Darlin PCD scheme, the equivalent density is about $d=1.5$.

\begin{table}[h!]
\caption{%
Recursion prover with and without inner sumcheck aggregation in terms of FFT operations and multi-scalar multiplications, for in-degree $\ell$ (without opening proof).
}
\label{t:InnerSumcheckAggregation}
\vspace*{3mm}
\centering
\begin{tabular}{|l|c|c|}
%Marlin with amortization of inner sumcheck (does not reflect base proofs)
% for these, a commit of 10 \cdot\max(n,m) is sufficient, yielding \approx 11*n
\cline{2-3}
\multicolumn{1}{c|}{} & polynomial arith. & commit
\\\hline
 intial round  &  $3 ~\textsf{FFT}(n)$   & $3~\textsf{MSM}(n)$
\\
\multirow{2}{*}{outer sumcheck
}  
	&  $2 ~\textsf{FFT}(n) + 2~\textsf{FFT}(2n)$
		& \multirow{2}{*}{$2 ~\textsf{MSM}(n) + 1~\textsf{MSM}(2n)$}
\\
	& $+ 3~\textsf{FFT}(3n)$
		&
%\\
%&& --  & $2 \cdot\max(n,m)$ 
\\
aggregation rounds 
	& $(4+\ell)~\textsf{FFT}(n)$
		& $(2+\ell)~\textsf{MSM}(n)$ 

\\\hline
overall 
	 &  $\approx  (18+ \ell)\cdot ~\textsf{FFT}(n)$ 
		& $\approx (9+\ell)~\textsf{MSM}(n)$
\\\hline\hline
without aggregation
	 &  $\approx  (18 + 5\cdot d)~\textsf{FFT}(n)$ 
		& $\approx (7+4\cdot d)~\textsf{MSM}(n)$
\\\hline
\end{tabular}
\end{table}

A more concrete comparison between Marlin and Darlin when merging $2$ previous proofs is provided in Table \ref{t:DarlinVsMarlin} from Section \ref{s:Introduction}.
There, the performance improvement over standard Marlin is estimated at $25\%$.
As our implementation is not ready yet, these timing estimates are based on a detailed simulation of a Darlin prover (in terms of MSM, FFT, vector and vector-matrix operations), run on an Amazon EC2 G4dn instance (with 4 Intel Xeon@2.5 GHz and 1 NVIDIA T4) currently offered at a rate of $0.526$ USD per hour.
The number of constraints for the verifier circuits for two previous proofs stem from detailed paper-and-pencil counts, where our circuit design follows the `deferred arithmetics' technique from \cite{Halo}, which postpones non-native arithmetic checks to the `next' circuit in recursion, in which these operations are again native. (We moreover apply their endomorphism-based scalar multiplication which reduces the number of constraints significantly.)
%In this table we consider a circuit for merging two previous proofs. 
In Table \ref{t:DarlinVsMarlin} we also take different segment sizes for the dlog commitment scheme into account. (See Section \ref{s:Segmentation} on segmentation of homomorphic polynomial commitment schemes.)
The strategy for choosing the segment sizes is as follows.
We start with segment size equal to $|H|=2^{19}$ (which is sufficiently large to cover the two verifier in circuit), and then reduce the committer key to $2^{18}$ and $2^{17}$.
% \footnotetext{%
% To optimize the number of constraints we follow \cite{Halo} and defer non-native arithmetic checks to the `next' circuit in recursion, in which these operations are again native.
% }%
As a consequence, the prover times decrease at the cost of increasing proof sizes and the number of constraints for the verifier circuit.
%\footnote{We excluded $2^{16}$ from the table as such small segment size would lead to a circuit beyond $2^{19}$ constraints.}.

%We further use segmentation for the dlog commitment scheme and tune its size to balance prover speed-up versus increased verifier circuits.
% Our constraint estimate for a \textit{Merge-$2$} circuit, which is the typical recursive circuit in our Darlin PCD scheme, stays below $2^{19}$ constraints while using a segment size of $2^{17}$, as can be seen in  Table \ref{t:DarlinVsMarlin}.
% There, the timing estimates are based on a careful simulation of a Darlin prover (in terms of MSM, FFT, vector and vector-matrix operations), run on an Amazon EC2 G4dn instance (with 4 Intel Xeon@2.5 GHz and 1 NVIDIA T4) currently offered at a rate of $0.587$ USD per hour.


\chapter{Future work}

We will implement Darlin as the recursive main argument of our upcoming Darlin proof carrying data suite \cite{DarlinFullProtocol}, using a $2$-cycle of ordinary elliptic curves such as the Pasta curves \cite{PastaCurves}.
The full suite will cover pure proof merging nodes (for in-degree $1$ and $2$) as well as special purpose nodes with additional consensus specific logic. 
Beyond that a separate transformation chain of arguments for converting Darlin proofs into ordinary Marlin proofs will be provided.
A formal description, including an in-depth security analysis will be given in \cite{DarlinFullProtocol}.


%, which form a 2-cycle of prime order elliptic curves over $255$ bit prime fields. 
%The Pasta curves come with a variety of practical properties, such as high 2-adicity of their scalar fields (as needed for FFT), the existence of a non-trivial curve automomorphism (to support endomorphism-based scalar multiplication), as well as the existence of field automorphisms of low algebraic degree (for arithmetic hash functions).
% We choose $x^5$-Poseidon \cite{Poseidon} as circuit-friendly hash function, with a suitable choice of rate and capacity. 
% In the design of recursive circuits we moreover apply all the optimizations from Halo \cite{Halo}: 
% \begin{itemize}
% \item
% We defer non-native arithmetics: 
% We split the Darlin verifier into its native and non-native part, where the non-native part is implemented in the `next' recursive circuit, in which these operations are again native. 
% This splittling technique is applied to the algebraic checks that come along with the Darlin verifier, as well as for the Poseidon-based Fiat-Shamir sponge.
% \item
% We use Halo's endomorphism-based scalar multiplication.
% \end{itemize}
% We further use segmentation for the dlog commitment scheme and tune its size to balance prover speed-up versus increased verifier circuits.
% Our constraint estimate for a \textit{Merge-$2$} circuit, which is the typical recursive circuit in our Darlin PCD scheme, stays below $2^{19}$ constraints while using a segment size of $2^{17}$, as can be seen in  Table \ref{t:DarlinVsMarlin}.
% There, the timing estimates are based on a careful simulation of a Darlin prover (in terms of MSM, FFT, vector and vector-matrix operations), run on an Amazon EC2 G4dn instance (with 4 Intel Xeon@2.5 GHz and 1 NVIDIA T4) currently offered at a rate of $0.587$ USD per hour.
% Given these timings we expect a proof tree for a base layer of overall $2^{27} = 128$ million constraints to run in less than $105$ seconds, assuming $256$ parallel SNARK provers and neglecting communication costs.
% Compared to a proof tree based on Marlin without inner sumcheck aggregation, the Darlin PCD scheme is expected to be faster by a factor of $0.74$.



% \chapter{The Darlin PCD scheme}
% \label{s:PCD}
% Zendoo \cite{Zendoo} is a cross chain transfer protocol allowing the exchange of assets between a Bitcoin-like \textit{mainchain} and \textit{sidechains} of different types, without knowing their internal structure (e.g., what consensus protocol is used, what types of transactions are supported).
% The coin transfers happening from sidechains to mainchain are performed by submitting to mainchain a special kind of transaction named \textit{certificate}, containing the list of sidechain-to-mainchain transfers (so-called `backward transfers') and a SNARK assuring their correctness (i.e. they followed the sidechain rules), optionally in zero-knowledge. 
% A \textit{Latus sidechain} is a specific Zendoo enabled sidechain structure which leverages recursive SNARKs to prove the correctness of backward transfers by validating the state of the sidechain (e.g., the state can be the collection of unspent transaction outputs).
% In order to assure practical performance and system decentralization, such succinct proof is computed in a distributed manner by several \textit{SNARK provers}. 
% The SNARK provers must follow the Latus incentive scheme \cite{LatusIncentive} which is specifically designed to enable decentralized proof creation and fair proving costs. 
% In this chapter we give a high-level description of the Darlin proof carrying data (PCD) scheme, which is the recursive scheme that serves such succinct proof of state within a Latus sidechain. 
% An in-depth discussion, including the recursive arithmetic circuits is given in the full protocol specifications \cite{DarlinFullProtocol}. 

% \begin{figure}
%     \caption{The Darlin PCD scheme and its subgraphs.
%     Normal Marlin base proofs $\pi_{base}$ are merged by Darlin nodes into block proofs $\pi_{block}$, which are incrementally added to the overall state of the sidechain.}
%     \label{f:PCDGraph}
%     \vspace*{0.5cm}
%     \centering
%     \includegraphics[width=0.9\textwidth]{DarlinPCD.pdf}
% \end{figure}

% \medskip
% The Darlin PCD scheme is based on \textit{Darlin}, i.e. the recursive argument from Section \ref{s:FullProtocol}, using a $2$-cycle of ordinary elliptic curves.
% Such cycle consists of two prime order elliptic curves $E_1=EC(F_1)$ and $E_2=EC(F_2)$ over prime fields $F_1$ and $F_2$ such that the order of the one group equals the field characteristic of the other.
% %
% The scheme applies a small collection of recursive circuits $\mathcal C(F_1)$ and $\mathcal C(F_2)$ over the two fields, used to process Darlin proofs along a (directed and acyclic) communication graph of nodes.
% %The Darlin proofs come with...
% The communication graph, as depicted in Figure \ref{f:PCDGraph}, is divided into three subgraphs:

% \begin{itemize}
%     \item 
%     The \textit{proof chain} is a sequential chain of proofs designed to keep track of the last sidechain state and its validating proof. 
%     The chain consists of a sequence of \textit{Add} and \textit{Wrap} nodes over the curves $E_1$ and $E_2$, used to merge the `previous' state proof and the `current' block proof in order to create a new proof certifying the validity of the state transitions derived by the block.
%     \item
%     The \textit{proof tree} is a dynamically arranged graph using \textit{Merge} nodes\footnotemark over the two curves $E_1$ and $E_2$ in order to end up with a unique block proof that is the result of merging all the base proofs.
%     \footnotetext{And their variants at the `bottom' border of the Darlin PCD scheme, the nodes which verify the normal Marlin proofs from the base layer.}
%     The finalizing \textit{Forge} node is a special purpose node which, in addition to proof merging, serves some additional (e.g. consensus related) logic.
%     \item 
%     The \textit{base layer} is composed by Marlin nodes over the curve $E_2$ which serve the \textit{base proofs} certifying the state transition for a dedicated fraction of block data. 
%     The proofs provided by these nodes validate both the common sidechain transaction rules and the custom logic needed by the specific sidechain.
% \end{itemize}
% Technically, the base layer forms a PCD scheme on its own, serving ordinary Marlin proofs which are then merged by the Darlin PCD into block proofs. 
% This hybrid design is a trade off between flexibility and performance.
% It allows to keep the number of circuits supported by the cross-circuit accumulator of the Darlin PCD scheme independent of the number of custom base proofs, and hence do not overstretch the memory usage of the SNARK provers.


% \medskip
% After a specified number of blocks (an \textit{epoch}) the sidechain commits its state (plus  transfers, beside some extra information collected over the epoch) to the mainchain, together with a succinct proof. 
% (For more details on the features of sidechain certificates, see \cite{Zendoo}.)
% To allow a mainchain verifier being agnostic of the circuits aggregated along the Darlin PCD, a separate \textit{exiting chain} of specific purpose nodes gradually transform a Darlin proof with its cross-circuit inner sumcheck accumulators into a normal Marlin proof (plus two deferred dlog accumulators). 
% The exiting chain, as illustrated in Figure \ref{f:ExitingChain} consists of three nodes.
% The first two nodes of the exiting chain prove correctness of the two cross-circuit aggregators (one over $E_1$, and the other one over $E_2$), by running an inner sumcheck argument for all the involved circuit matrices.
% Technically, these nodes run a cross-circuit variant of Marlin, called \textit{Rainbow Marlin}, which we describe in detail in our full protocol specification \cite{DarlinFullProtocol}. 
% The third node of the exiting chain is again a normal Marlin node, which verifies the succinct part of the previous Rainbow Marlin proof; 
% the dlog accumulators of the two previous Rainbow Marlin proofs are simply passed via public inputs. 
% The final proof is a normal Marlin proof, together with two dlog accumulators (one over $E_1$, and one from $E_2$). 

% \begin{figure}[h!]
%     \caption{The exiting chain of the Darlin PCD scheme gradually transforms a Darlin proof into a normal Marlin proof.}
%     \label{f:ExitingChain}
%     \vspace*{0.3cm}
%     \centering
%     \includegraphics[width=0.5\textwidth]{ExitingChain.pdf}
% \end{figure}

% \chapter{Implementation}
% \label{s:Implementation}

% We instantiate our Darlin PCD scheme using the Pasta curves \cite{PastaCurves}, which form a 2-cycle of prime order elliptic curves over $255$ bit prime fields. 
% The Pasta curves come with a variety of practical properties, such as high 2-adicity of their scalar fields (as needed for FFT), the existence of a non-trivial curve automomorphism (to support endomorphism-based scalar multiplication), as well as the existence of field automorphisms of low algebraic degree (for arithmetic hash functions).
% We choose $x^5$-Poseidon \cite{Poseidon} as circuit-friendly hash function, with a suitable choice of rate and capacity. 
% In the design of recursive circuits we moreover apply all the optimizations from Halo \cite{Halo}: 
% \begin{itemize}
% \item
% We defer non-native arithmetics: 
% We split the Darlin verifier into its native and non-native part, where the non-native part is implemented in the `next' recursive circuit, in which these operations are again native. 
% This splittling technique is applied to the algebraic checks that come along with the Darlin verifier, as well as for the Poseidon-based Fiat-Shamir sponge.
% \item
% We use Halo's endomorphism-based scalar multiplication.
% \end{itemize}
% We further use segmentation for the dlog commitment scheme and tune its size to balance prover speed-up versus increased verifier circuits.
% Our constraint estimate for a \textit{Merge-$2$} circuit, which is the typical recursive circuit in our Darlin PCD scheme, stays below $2^{19}$ constraints while using a segment size of $2^{17}$, as can be seen in  Table \ref{t:DarlinVsMarlin}.
% There, the timing estimates are based on a careful simulation of a Darlin prover (in terms of MSM, FFT, vector and vector-matrix operations), run on an Amazon EC2 G4dn instance (with 4 Intel Xeon@2.5 GHz and 1 NVIDIA T4) currently offered at a rate of $0.587$ USD per hour.
% Given these timings we expect a proof tree for a base layer of overall $2^{27} = 128$ million constraints to run in less than $105$ seconds, assuming $256$ parallel SNARK provers and neglecting communication costs.
% Compared to a proof tree based on Marlin without inner sumcheck aggregation, the Darlin PCD scheme is expected to be faster by a factor of $0.74$.

%\medskip
% Our implementation rests on arkworks \cite{arkworks}, an ecosystem of general purpose libraries for zk-SNARKs. 
% Coboundary Marlin is derived from their Marlin implementation, and we use their R1CS synthesizer as well as a large collection of subcircuits (called `gadgets').
% Our GPU prover used in the benchmarks in Table \ref{t:DarlinVsMarlin} relies on the Filecoin \cite{Filecoin} implementation for FFT and multi-scalar-multiplications.
% We plan to finish the implementation of our PCD scheme in the upcoming months.  
% Concrete benchmarks will then be given in an updated version of this document.

\chapter{Acknowledgements}

The first author is indebted to Maus and Bowie for their appreciated feedback.
Without them, the main recursive argument would miss its most important feature, the whisker feedback loop in the cross-meal aggregation of fish, chicken and beef. 
One of the first readers is also grateful to Peperita, that helped moving away from pairings in exchange for tasty kibble.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\bibliographystyle{alpha}
\bibliography{bibfileSNARKs}

\appendix
%\newpage
\chapter{Appendix}
\label{s:Appendix}


%\section{Formal analysis of Protocol \ref{p:CompleteArgument}}

% Plan
% \begin{enumerate}
%     \item
%     Basic notions: probabilistic polynomial time and expected polynomial time algorithms, negligible, binary polynomial decidable relation, indexed relations.
%     \item
%     Interactive arguments for binary relations $\mathcal R$ (using common reference string), and the security properties completeness, (perfect honest verifier) zero-knowledge, and witness-extended simulation (why not knowledge soundness?), 
%     \item
%     Universal setup preprocessing SNARK for indexed relations as a special case, in which the setup consists by two algorithms (one for the CRS, and the other one for the index)
%     \item
%     Polynomial commitment schemes with $\open$, $\verify$ as interactive argument, plus additional security properties binding and hiding.
% \end{enumerate}


\section{Notation}

%An algorithm is a Turing machine 
We denote the security parameter by $\lambda$, where we throughout consider it in unary representation. 
A function $f(\lambda)$ is \textit{negligible} if for every polynomial $p(\lambda)$, it holds that $\lim_{\lambda\rightarrow\infty}f(\lambda)\cdot p(\lambda)= 0$, or in short $f(\lambda)=o(\nicefrac{1}{p(\lambda)})$. 
%Two functions $f(\lambda)$ and $g(\lambda)$ are \textit{computationally indistinguishable}, denoted by $f\approx g$, if their difference is negligible in $\lambda$.


Probabilistic algorithms are denoted by capital letters $\mathsf A, \mathsf B$, etc., and we write $y\leftarrow \mathsf A(x)$ if an algorithm $\mathsf A$ ouputs a string $y$ given an input string $x$ while using some internal random coins $r$ uniformly sampled from $\{0,1\}^*$.
Whenever we need to refer to the used random coins $r$, we shall explicitly write $y=\mathsf A(x;r)$. 
We say that $\mathsf A$ is \textit{probabilistic polynomial time (p.p.t.)}, if its run time $T_{x,r}$ on input $x$ and internal random coins $r$ is bounded by some fixed polynomial $p(|x|)$ independent of the random coins, where $|x|$ denotes the length of its input.
We say that $\mathsf A$ is \textit{expected polynomial time} if the expected run time $E(T_{x,r})$, where the expectation is taken over all random coins $r$, is bounded by some polynomial in the length of the input.
%is a Turing machine which, given a string $x$ as input and some uniformly sampled random string $r\sample\{0,1\}^*$ outputs a string $y$, in short $y\leftarrow \mathsf A(x)$.
%We call $r$ the internal random coins, and write $y= \mathsf A(x;r)$ whenever appropriate.
The interaction of two interactive probabilistic algorithms $\mathsf A$ and $\mathsf B$ is denoted $\langle \mathsf A, \mathsf B\rangle$, where we explicitly clarify what are the inputs and outputs of both algorithms. 



%Assume that $\mathcal R$ is a polynomial decidable binary relation. 
%We write... 

\section{Interactive arguments}
\label{s:ArgumentSystems}

% Let $\mathcal R$ be a relation generator, which given the security parameter $\lambda$ (in unary representation) outputs a polynomial time decidable binary relation $R\leftarrow\mathcal R(\lambda)$.
% Any pair $(x,w)\in R$ consists of a statement $x$ and a witness $w$, and we denote by $\mathcal R_\lambda$ the range of all possible relations output by $\mathcal R$ for fixed $\lambda$.  
% Throughout this paper we assume that $\mathcal R$ generates relations for rank-one constraint systems (R1CS) of size $N$ over some finite field $F$, and as such it is comprised of (or, is indexed by) the subrelations defined by (quadratic) matrices $A,B,C\in F^{N\times N}$, i.e.
% \begin{equation*}
% R_{A,B,C} = \{(x,w): y=(x\|w)\in F^N~\text{satisfies}~ (A\cdot y)\odot(B\cdot y) = C\cdot y\}.
% \end{equation*}
% Whenever convenient we shall write $R$ for the relation $R_{A,B,C}$ or for the matrix triple $(A,B,C)$.

%Here $x\in F^{\ell}$ is the public input of the circuit, $w\in F^{N-\ell}$ is its internal state vector, and $A,B,C\in F^{N\times N}$ are the (padded) matrices of the constraint system.

Let $\mathcal R$ be a polynomial time decidable binary relation.
An interactive argument system for $\mathcal R$ consists of three probabilistic polynomial time algorithms 
\[
(\setup, \prove, \verify).
\]
%
Given the security parameter $\lambda$ in unary representation, $\setup(\lambda)$ outputs a common reference string $crs$ which supports all statement-witness pairs $(x,w)$ up to a certain maximum length $N=N(\lambda)$, which we write in short $(x,w)\in\mathcal R_N$.
%We will use a simple non-interactive setup procedure without any trust assumption, but in general $\setup$ might be a secure multi-party protocol.
Given $(x,w)\in\mathcal R_N$, the algorithms $\prove$ and $\verify$ are used to interactively reason about whether $x$ belongs to the language defined by $\mathcal R$ or not.
We denote their interaction by $tr\leftarrow\langle \prove(x,w), \verify(x)\rangle$ with $tr$ as the transcript of the interaction, and we assume that both algorithms have access to the $crs$ without explicitly declaring them as inputs.
After at most polynomially many steps the verifier accepts or rejects, and we say that $tr$ is accepting or rejecting.


%
% In a pre-processing `offline' phase, given a relation $R\in\mathcal R_\lambda$, $(pk,vk)\leftarrow\keygen(R)$ computes the prover and verifier key for $R$.
% In our situation $pk$ and $vk$ are always generated in a deterministic manner from $R$, hence $(pk,vk)=\keygen(R)$. 
% Together with $crs$, these keys are used in the `online' phase to reason about whether an instance $(x,w)$ belongs to $R$ or not. 
%
% We denote by $\langle \prove(pk,x,w), \verify(vk,x)\rangle$ the interaction of the interactive algorithms $\prove$ and $\verify$, and by $tr$ the transcript of that interaction. 
% After at most polynomially many steps the verifier accepts or rejects.

\begin{defn}[Perfect completeness]
\label{def:Completeness}
An interactive argument system $(\setup$, $\prove,\verify)$ satisfies  \textit{perfect completeness} if
\begin{equation*}
\prob{
\begin{minipage}{3cm}
$\langle\prove(x,w),\verify(x)\rangle$ \text{ is accepting } 
\end{minipage}
\:\left|\: 
\begin{minipage}{3.2cm}
	$crs\leftarrow\setup(\lambda)$, 
	\\
	$(x,w)\leftarrow\mathcal A(\lambda)$, with $(x,w)\in\mathcal R_N$
\end{minipage}
\right.
} 
= 1.
\end{equation*}
\end{defn}

%%%
%%%     The Bellare Goldreich definition as used by Halo Infinite
%%%
% This is the \cite{PoK} definition without knowledge errors.
% \begin{defn}[Knowledge-soundness]
% An interactive argument system $(\setup$, $\prove,\verify)$ for the relation $\mathcal R$ is \textit{knowledge sound} if for every $x$ from $\mathcal L_\mathcal R$ and every adversary $\mathcal A$ which makes $\langle\mathcal A,\verify(x)\rangle$ accept with non-negligible probability $\varepsilon(x) >\negl[\lambda]$, there is an expected polynomial time algorithm $\mathcal E=\mathcal E^\mathcal A$ with blackbox access to $\mathcal A$ which does on average $\poly[|x|,\lambda]/\varepsilon$ calls, and overwhelmingly outputs a witness $w$ such that  $(x,w)\in\mathcal R$.
% \end{defn}

% Avoiding composition problems due to expected polynomial time: 
% Truncating to strict polynomial time still conserving the expected ~1/varepsilon calls does not work. 
% This is the reason why we use the Halevi-Micali definition, which is relaxed in this regard.

We define knowledge-soundness in the style of \cite{PoKHaleviMikali}.
However we do not dwell on the structure or the message distribution of the blackbox extractor. 
The reason for this choice of definition is the modularity of our proof of Theorem \ref{thm:CompleteProtocol}, which refers to the security result on the batch evaluation argument from \cite{HaloInfinite}.
%We use the \cite{PoKHaleviMikali} definition of knowledge-soundness without knowledge errors and without explicit separation of the extractor into a transcript sampler and 
\begin{defn}[Knowledge-soundness]
\label{def:KnowledgeSoundness}
An interactive argument system $(\setup$, $\prove,\verify)$ for the relation $\mathcal R$ is \textit{knowledge sound} if for every $x$ from $\mathcal L_\mathcal R$ and every adversary $\mathcal A$ which makes $\langle\mathcal A,\verify(x)\rangle$ accept with non-negligible probability $\varepsilon(x) >\negl[\lambda]$, there is a strict polynomial time algorithm $\mathcal E=\mathcal E^\mathcal A$ with blackbox access to $\mathcal A$ which does at most $\poly[|x|,\lambda]/\poly[\varepsilon]$ calls, and overwhelmingly outputs a witness $w$ such that  $(x,w)\in\mathcal R$.
\end{defn}


%%%
%%% towards a composition robust notion of knowledge-soundness
%%%
% We can modify/extend our strict polynomial time extractor (from the proof of our main thm) to serve indistinguishable transcripts, but the probability of producing a valid witness will differ noticeably from the acceptance probability. 
% This is due to the particular manner of constructing a strict p.t.t. extractor, which is obtained by truncating the classical expected polynomial time transcript sampler. 
% Truncating without changing the success probability noticeably does not seem possible.
% The corresponding property in the style \cite{Lindell, NonBlackBoxExtractor} would be as follows.
% As \cite{NonBlackBoxExtractor} we demand strict p.p.t. emulators. 
% The difference to \cite{NonBlackBoxExtractor} is that we allow the emulator to fail in producing a valid witness, yet it still succeeds with noticeably probability if the adversary does.
%
% \begin{defn}[Weak witness-extended emulation]
% \label{def:WitnessExtendedEmulation}
% An interactive argument system $(\setup,\prove,\verify)$ has \textit{weak witness-extended emulation} if for every p.p.t. adversary $\mathcal A$ and every $c>0$  there is a strict(!) polynomial time algorithm $\mathcal E=\mathcal E^\mathcal A$ with blackbox access to the next-message functions of $\mathcal A$ such that 
% \begin{multline*}
% \prob{
% \begin{minipage}{2.5cm}
% $tr' \text{ is accepting}$
% \\
% $\wedge (x,w)\notin\mathcal R$
% \end{minipage}
% \:\left|\: 
% \begin{minipage}{2.6cm}
% 	$crs\leftarrow\setup(\lambda)$, 
% 	\\
% 	$x\leftarrow\mathcal A(crs)$ ,
% %	with  $R\in R_\lambda$, 
% 	\\
% 	$(w,tr')\leftarrow\mathcal E^{\mathcal A}$
% \end{minipage}
% \right.
% } 
% \\
% \leq \frac{1}{\lambda^c}\cdot 
% \prob{
% tr \text{ is accepting}
% \:\left|\: 
% \begin{minipage}{2.8cm}
% 	$crs\leftarrow\setup(\lambda)$, 
% 	\\
% 	$x\leftarrow\mathcal A_1(crs)$, 
% %	with  $R\in R_\lambda$, 
% 	\\
% 	$tr\leftarrow \langle \mathcal A_1, \verify(x)\rangle$
% \end{minipage}
% \right.
% }
% + \negl[\lambda]
% ,
% \end{multline*}
% and $tr'$ as output by $\mathcal E$ is computationally indistinguishable from $tr$.
% \end{defn}
% 


%%%
%%% The original expected polynomial time definition as from \cite{Lindell}. 
%%%
% \begin{defn}[Witness-extended Emulation]
% \label{def:WitnessExtendedEmulation}
% An interactive argument system $(\setup,\prove,\verify)$ has \textit{witness-extended emulation} if for every p.p.t. adversary $\mathcal A$ and every polynomial time distinguisher $D$ there is an expected polynomial time algorithm $\mathcal E$ such that
% \begin{multline*}
% \prob{
% \begin{minipage}{5.2cm}
% $D(tr') = 1$ 
% $\wedge$ 
% \\
% $(\text{if } tr' \text{ accepts, then } (x,w)\in\mathcal R_N)$ 
% \end{minipage}
% \:\left|\: 
% \begin{minipage}{3.2cm}
% 	$crs\leftarrow\setup(\lambda)$, 
% 	\\
% 	$x\leftarrow\mathcal A_1(crs)$ ,
% %	with  $R\in R_\lambda$, 
% 	\\
% 	$(w,tr')\leftarrow\mathcal E^{\mathcal A}$
% \end{minipage}
% \right.
% }
% \\
% \approx
% \prob{
% D(tr) = 1
% \:\left|\: 
% \begin{minipage}{2.8cm}
% 	$crs\leftarrow\setup(\lambda)$, 
% 	\\
% 	$x\leftarrow\mathcal A_1(crs)$, 
% %	with  $R\in R_\lambda$, 
% 	\\
% 	$tr\leftarrow \langle \mathcal A_1, \verify(x)\rangle$
% \end{minipage}
% \right.
% }.
% \end{multline*}
% \end{defn}

\begin{defn}
\label{def:ArgumentOfKnowledge}
We say that an interactive argument system $(\setup, \prove,\verify)$ is an \textit{argument of knowledge}, if it is perfectly complete and knowledge sound as defined above.
It is said to be \textit{succinct}, if the size of the transcript is sublinear in the size of $(x,w)\in\mathcal R$.
\end{defn}

\medskip
As we do not require any trust assumptions for the setup, our definition of zero-knowledge does not make use of trapdoors. 

\begin{defn}[Perfect honest verifier zero-knowledge]
\label{def:ZeroKnowledge}
An interactive argument system $(\setup, \prove, \verify)$  is \textit{perfect honest verifier zero-knowledge} if there is a p.p.t. algorithm $\simulate$ %with access to the common reference string and 
such that for every p.p.t. algorithm $\mathcal A$, 
\begin{multline*}
    \prob{
        \begin{minipage}{2cm}
            \centering
            $(x,w)  \in\mathcal R_N$ 
            \\
            $\wedge$
            \\
            $\mathcal \mathcal A(tr') = 1$ 
        \end{minipage}
        \:\left|\: 
        \begin{minipage}{2.7cm}     
        	$crs\leftarrow\setup(\lambda)$, 
	        \\
	        $(x,w)\leftarrow \mathcal A(crs)$,
	        \\
	        $tr'\leftarrow\simulate(crs,x)$
        \end{minipage}
        \right.
    }
    \\
    =
    \prob{
        \begin{minipage}{2cm}
            \centering
            $(x,w)  \in\mathcal R_N$ 
            \\
            $\wedge$
            \\
            $\mathcal A(tr) = 1$ 
        \end{minipage}
        \:\left|\: 
        \begin{minipage}{4cm}     
        	$crs\leftarrow\setup(\lambda)$, 
	        \\
	        $(x,w)\leftarrow \mathcal A(crs)$,
	        \\
	        $tr\leftarrow\langle\prove(x,w),\verify(x)\rangle$
        \end{minipage}
        \right.
    }.
\end{multline*}
\end{defn}



\section{Forking Lemmas}

We use the forking Lemma from \cite{BootleGroth} and we obtain strict polynomial time of the sampling algorithm by truncation. 
Assume that $(\setup$, $\prove,\verify)$ is an $(2r+1)$-move public-coin argument, by which we mean that in each round the verifier messages are chosen uniformly at random from a sample space $S$.
%(We throughout assume that the size of $S$ grows super-polynomially in the security parameter $\lambda$.)
Given a transcript $tr$ resulting from the interaction of $\mathcal A(\:.\:)$ with the verifier $\verify(crs,x)$, we denote by $tr|_{\leq i}$, with $i=0,\ldots,r$, the partial transcript consisting of the prover and verifier messages of the first $2i+1$ moves. 
An \textit{$(n_1,\ldots,n_r)$-tree of accepting transcripts} is a tree of depth $r$ which is rooted in a prover's first message $tr|_{\leq 0}$ and in which each node at level  $i\in\{0,\ldots,r-1\}$ represents a partial transcript $tr|_{\leq i-1}$ and has exactly $n_i$ children nodes extending this transcript. 
%where the next round verifier challenges of these extensions are pairwise different.
The tree has overall $K(\lambda)=\prod_{i=1}^r n_i$ leafs standing for complete transcripts in which the verifier eventually accepts.
We assume that the size of $S$ grows superpolynomially in $\lambda$, so that 
\[
\prob{x_1\neq x_2  | x_1,x_2\sample S} > 1-\negl[\lambda].
\]
\begin{lem}[\cite{BootleGroth}]
\label{lem:ForkingLemma}
Let $(\setup,\prove,\verify)$ be a $(2r+1)$-move public-coin interactive proof, and $\mathcal A$ a p.p.t. adversary which runs in expected time $t_\mathcal A$ and succeeds $\langle\mathcal A(\:.\:),\verify(crs,x)\rangle$ with non-negligible probability $\varepsilon= \varepsilon(x)$ on public input $x$.  
If $n_1,\ldots,n_r\geq 2$ are such that $K(\lambda)=\prod_{i=1}^r n_i$ is polynomially bounded, then there exists a p.p.t. algorithm $\mathcal T$ that calls the next message function of $\mathcal A$ at most $2\cdot K(\lambda)/\varepsilon$ times and with non-negligible probability $\varepsilon/2$ outputs an $(n_1,\ldots,n_r)$-tree of accepting transcripts in which all pairs of sibling-node challenges $x_1,x_2$ are subject to $x_1\neq x_2$.
%$\mathcal T$ calls 
%The expected running time of $\mathcal T$ is $\poly[\lambda]\cdot\frac{t_\mathcal A}{\varepsilon}$.
\end{lem}
 
%%%
%%% extended forking lemma with general constraints on the challenges
%%%
% The constraints imposed on the challenges in a tree of accepting transcripts are formalized by $\varphi_i:S\times S\rightarrow \{0,1\}$,  ($i=1,\ldots,r$, i.e. one for each round), such that 
% \[
% \prob{\varphi_i(x_1,x_2) = 1 | x_1,x_2\sample S} > 1-\negl[\lambda].
% \]

% \begin{lem}[\cite{BootleGroth},\cite{HaloInfinite}]
% \label{lem:ForkingLemma}
% Let $(\setup,\prove,\verify)$ be a $(2r+1)$-move public-coin interactive proof, and $\mathcal A$ a p.p.t. adversary which runs in expected time $t_\mathcal A$ and succeeds $\langle\mathcal A(\:.\:),\verify(crs,x)\rangle$ with non-negligible probability $\varepsilon= \varepsilon(x)$ on public input $x$.  
% If $r=r(\lambda)$ and $n_1,\ldots,n_r$ are such that $K(\lambda)=\prod_{i=1}^r n_i$ is polynomially bounded, then there exists a p.p.t. algorithm $\mathcal T$ that calls the next message function of $\mathcal A$ at most $2\cdot K(\lambda)/\varepsilon$ times and with non-negligible probability $\varepsilon/2$ outputs an $(n_1,\ldots,n_r)$-tree of accepting transcripts.
% Every transcript tree output by $\mathcal T$ is such that for all $i\in\{1,\ldots, r\}$ all pairs of sibling-node challenges $x_1,x_2$ are subject to $\varphi_i(x_1,x_2)=1$.
% %$\mathcal T$ calls 
% %The expected running time of $\mathcal T$ is $\poly[\lambda]\cdot\frac{t_\mathcal A}{\varepsilon}$.
% \end{lem}



For the sake of completeness we shortly sketch the construction of $\mathcal T$ as claimed in Lemma \ref{lem:ForkingLemma}.
The tree finder algorithm $\mathcal T'$ from \cite{BootleGroth} is a rejection sampler which is allowed to fail at every first completion of $\langle\mathcal A,\verify(crs,x)\rangle$ on a given partial transcript $tr|_{\leq i}$.
It succeeds with probability 
\begin{align*}
p_{tr|_{\leq i}} &= \prob{ \mathcal T' ~\text{completes subtree for } tr|_{\leq i}} 
\\
&= \prob{\langle\mathcal A(\:.\:),\verify(crs, x)\rangle ~\text{succeeds on } tr|_{\leq i} }.
\end{align*}
and runs in expected polynomial time
\[
\expect{\#\text{ of }\mathcal A \text{ calls given } tr|_{\leq i}}  \leq n_i\cdot n_{i+1}\cdot\ldots\cdot n_r.
\]
Overall $\mathcal T'$ is of expected polynomial time calling  $\mathcal A$ at most $K(\lambda)$ times on average, and $\mathcal T'$ succeeds in producing a complete $(n_1,...,n_r)$-tree of accepting transcript with probability $\varepsilon$. 
The probability that such a complete tree of accepting transcripts has collisions (i.e. two sibling challenges coincide) is negligible, see the full version of \cite{BootleGroth}.
Finally limiting the run time of $\mathcal T'$ to $2\cdot K(\lambda)/\varepsilon$ calls of $\mathcal A$ (and returning $\bot$ in that case) yields a strict polynomial time algorithm  which still succeeds with a probability of at least $\varepsilon/2$.
%Finally, amplifying that algorithm by repeating it at most $2/\varepsilon$ times yields the claimed $\mathcal T$ from Lemma \ref{lem:ForkingLemma}. 

% The expected polynomial time tree finder $\mathcal T'$ comes with another useful property:
% If we choose $\delta$ small enough then an overwhelming fraction of successful trees $tree\leftarrow\mathcal T'$ is such that on each partial transcript $tr\|_{\leq i}$ in the tree $\mathcal A$ succeeds with probability of at least $\delta$. 
% We can even choose $\delta$ to be noticable in $\lambda$. 
% This fact holds, as otherwise the rejection sampler would take more than $c/\delta$ time on a set noticable enough to contradict the expectation is bounded by $K(\lambda)\cdot (1/\varepsilon)$ calls of $\mathcal A$. 
% Therefore we may assume that the stopped and amplified algorithm from Lemma \ref{lem:ForkingLemma} is such that an overwhelming fraction of successful trees satisfies this $\delta$-property. 

%\medskip
% We need this Lemma as our knowledge-soundness notion is non-uniform with respect to the instance x from the language.
%As \cite{HaloInfinite} we use the following coarse estimate on the probabilities for succeeding in continuing partial transcripts as output by the tree sampler $\mathcal T$. 

\begin{lem}[\cite{HaloInfinite}]
\label{lem:ConditionalProbability}
Let $\delta$ be such that $0 < \delta \leq \frac{\varepsilon^2}{8 K(\lambda)}$.
Then with probability at least $\varepsilon/4$ the tree finding algorithm $\mathcal T$ from Lemma \ref{lem:ForkingLemma} outputs a tree of accepting transcripts with the following property: 
For every partial transcript $tr|_{\leq i}$ of length $i$ in the tree, the conditional success probability for $\mathcal A(\:.\:)$ continuing the partial transcript $tr|_{\leq i}$ is at least $\delta$.
% \[
% \prob{ 
%     \left.
%         \begin{minipage}{4cm}
%         \centering
%         tr_0'=tr_0  
%         \\
%         \wedge
%         \\
%         (tr_0'\|tr_1') \text{ is accepting } 
%         \end{minipage}
%     \right| 
%     (tr_0'\|tr_1')\leftarrow \langle A(\:.\:),\verify(crs,x)\rangle}
% \]
\end{lem}
\begin{proof}
The tree sampler $\mathcal T$ from Lemma \ref{lem:ForkingLemma} tests at most $2\cdot K(\lambda)/\varepsilon$ partial transcripts $tr|_{\leq i}$. 
Such transcript succeeds with the probability $p_{tr|_{\leq i}}$ as above.
Therefore, the probability that one of these $p_{tr|_{\leq i}}$ is smaller than a given $\delta$ is at most $2\cdot K(\lambda)/\varepsilon \cdot\delta$.
Choosing the latter of at most $\varepsilon/4$ yields the assertion of the lemma.
\end{proof}

We note that the factors $1/2$ in Lemma \ref{lem:ForkingLemma} and $1/8$ in Lemma \ref{lem:ConditionalProbability} are arbitrary. 
Any other choice of these factors $>1-1/\poly[\lambda]$ is possible.

% % \begin{lem}
% % Every algorithm $\mathcal A$ which runs in expected time $t\in\poly[\lambda]$ and which has
% %  non-negligible success probability can be turned into a strictly polynomially bounded algorithm with non-negligible success probability.
% % \end{lem}
% % \begin{proof}
% % Let $T$ be the running time of $\mathcal A$, and let $\varepsilon(\lambda)$ its probability of success. 
% % Then $\varepsilon(\lambda)>1/p(\lambda)$ for some polynomial $p(\lambda)$.
% % By the Markov inequality, $P(T\geq a\cdot t)\leq \frac{t}{a\cdot t} = \frac{1}{a}$, hence choosing $a= 2\cdot p(\lambda)$ yields $p(T\geq a\cdot t)\leq 1/(2\cdot p(\lambda))$.
% % By stopping $\mathcal A$ when reaching time $a(\lambda)\cdot t(\lambda)$ (and output $\bot$ in this case) we obtain a strictly bounded algorithm with success probability $\varepsilon'>1/(2p)$.
% % \end{proof}






\section{Proof of Theorem \ref{thm:CompleteProtocol}}
\label{s:ProofCompleteArgument}

Theorem \ref{thm:CompleteProtocol} is subject to Protocol \ref{p:CompleteArgument}, extended by the verification of the resulting $acc''=(\gamma,H'',[T_{H''}(\gamma,Y)])$. 
We refer to this extended protocol as the full protocol. 
The batch evaluation proof $\mathsf{Eval}$ is regarded as a subprotocol, and we use knowledge soundness and honest verifier zero-knowledge of it (Theorem \ref{thm:BatchEvaluation} and Theorem \ref{thm:dlog}) to infer the same properties for the full protocol.
% is 
% is a perfectly honest verifier zero-knowledge argument of knowledge for the relation 
% \begin{multline*}
% \mathcal R = \big\{((\mathcal C,k, acc_\mathcal C',acc_{dlog}', x),w) :  (x,w)\in R_{\mathcal C_k}
% \wedge \phi(acc_{\mathcal C}')=1
% \\
% \wedge  \phi_{dlog}(acc_{dlog}')=1 
% %\\
% \wedge (acc_C',acc'_{dlog}) \text{ is consistent with }x
% \big\},
% \end{multline*}
% where $\mathcal C=\{\mathcal C_1,\ldots,C_M\}$ is a collection of arithmetic circuits over $F$, $k\in\{1,\ldots,M\}$, and $R_{\mathcal C_k}$ is the R1CS relation for $\mathcal C_k$.
%$(A_k\cdot y)\odot(B_k\cdot y) = C_k\cdot y$ where $y=(x\|w)$.



\subsection{Knowledge soundness}

Assume maximum degree for the polynomial commitment scheme is $d=\poly[\lambda]$ with $d\geq 2\cdot n + b$.
The proof is divided into two steps. 
In the first one, we show special soundness of the algebraic oracle proof.
The second step uses the transcript sampler from Lemma \ref{lem:ForkingLemma} to construct a strict polynomial time extractor from the strict polynomial time extractor of the batch evaluation argument. 


\subsubsection{Step 1. Special soundness.} 
Consider the protocol as an interactive oracle proof where the oracles are guaranteed having a degree of at most $d=\poly[\lambda]$.
Besides the arithmetic checks on the evaluation claims, the verifier checks the oracle for $T_{H''}(\gamma,Y)$ by reading it in full length\footnote{%
Equivalently, the verifier may query the oracle at $d+1$ different points and reconstructs the polynomial from the values.
}
and compare it against the polynomial described by $H''$ as computed in the protocol.
We claim that this `algebraic oracle proof' is $(m_1, m_2, m_3, m_4)$-special sound, with
\[
(m_1,m_2,m_3,m_4,m_5)= (3, n, 2\cdot d + 1, 2, d+1),
\]
in the following sense: 
Given an $(m_1,m_2,m_3,m_4,m_5)$-tree of accepting transcripts with pairwise distinct verifier challenges for $\eta\in F,\alpha,\beta\in F\setminus H, \lambda\in F, \gamma\in F$, respectively, then
\begin{itemize}
\item
the polynomial in the oracle from $acc_{dlog}'$ is the claimed reduction polynomial $h(\vec\xi',X)$,
\item 
the oracles intended for $T_{H'}(\gamma',Y)$ and $T_{\eta}(\alpha,Y)$ in fact carry the correct polynomials, and
\item
the polynomial $y(X)= x(X)+(X^\ell -1)\cdot\hat w(X)\bmod (X^n-1)$ with $\hat w(X)$ from $[\hat w(X)]$ satisfies the R1CS identities \eqref{e:QAPc}, \eqref{e:QAPa}, \eqref{e:QAPb}.% and hence define a solution $y=(x\|w)$ of the R1CS for $\mathcal C_k$.
\end{itemize}
This is true for the following reasons:
\begin{itemize}
\item
$m_1=3$ pairwise distinct $\eta_1,\eta_2,\eta_3\in F$ are sufficient to derive the R1CS identities \eqref{e:QAPc}, \eqref{e:QAPa}, \eqref{e:QAPb} from the `lincheck' identity \eqref{e:lincheck}.
(The Vandermonde matrix for different choices of $\eta$ is invertible.)

\item
$m_2= n$ pairwise distinct $\alpha_1,\ldots,\alpha_n\in F\setminus H$ allow inverting the reduction of the lincheck identity \eqref{e:lincheck} to the sumcheck identity \eqref{e:OuterSumcheck} by means of the Lagrange kernel.
(Recall that the sumcheck is obtained from the lincheck by applying $\langle L_n(X,\alpha), \,.\,\rangle$. 
By Lemma \ref{lem:LagrangeKernel} the inner products for any $n$ different values of $\alpha$ allow to uniquely reconstruct the lincheck polynomial modulo $(X^n-1)$.
)

\item
$m_3=2\cdot d+1$ pairwise distinct $\beta_1,\ldots,\beta_{2d+1}\in F\setminus H$ are sufficient 
to infer the coboundary outer sumcheck identity \eqref{e:OuterSumcheckCoboundary} and hence  \eqref{e:OuterSumcheck} on the full domain $F$, as well as the identities for the first step of the inner sumcheck aggregation.
(These identities are of degree at most $d$.)

\item
$m_4=2$ distinct $\lambda_1,\lambda_2\in F$ allow for reconstructing the component polynomials from their linear combination $T_{\vec\eta}(X,\beta) + \lambda\cdot T_{H'}(X,\beta)$ in the second step of the inner sumcheck aggregation.
(Again, since the Vandermonde matrix is invertible.)

\item
and $m_5=d+1$ pairwise distinct $\gamma_1,\ldots,\gamma_{d+1}\in F$ are sufficient to infer the correctness of the polynomial behind the linear combination $[T_{\vec\eta}(X,\beta)]+\lambda\cdot [T_{H'}(X,\beta)]$. %, and the polynomial behind the oracle in $acc_{dlog}'$.
(Again, the polynomial is of degree at most $d$.)
\end{itemize}

For the sake of completeness let us clarify the polynomial identities involved in the two aggregation steps. %and oracle proofs in general.
In the second step the identity to be checked is the \textit{aggregation identity}
\begin{equation}
\label{e:AggregationIdentity}
q(X) + \lambda\cdot q'(X) = T_{H''}(X,\beta)
\end{equation}
where $q(X)$ and $q'(X)$ are the polynomials behind the oracles for $T_\eta(X,\beta)$ and $T_{H'}(X,\beta)$, respectively.
Likewise, in the first aggregation step the identities are the simple \textit{matching identities}
\begin{align}
\label{e:MatchingIdentity1}
p(Y) &= T_\eta(\alpha,Y),
\\
\label{e:MatchingIdentity2}
p'(Y) &= T_{H'}(\alpha',Y),
\end{align}
where $p(Y)$ and $p'(Y)$ are the polynomials behind the oracles for $T_\eta(\alpha,Y)$ and $T_{H'}(\alpha',Y)$.
%
%Let us also point out an important aspect of the verification of \eqref{e:AggregationIdentity}, \eqref{e:MatchingIdentity1} and \eqref{e:MatchingIdentity2}.
Note that all three identities  \eqref{e:AggregationIdentity}, \eqref{e:MatchingIdentity1},  \eqref{e:MatchingIdentity2} are assured by the verifier \textit{without} computing the values on the right hand sides for the respective random challenges $X=\gamma$ and $Y=\beta$.
In case of the aggregation identity \eqref{e:AggregationIdentity}, the verifier reads the polynomial behind $[T_{H''}(\gamma,Y)]$ in full length and checks it to be equal to $T_{H''}(\gamma,Y)$.
If this is the case, then its value provided for $Y=\beta$, which the opening of $[q(X)]+\lambda\cdot [q'(X)]$ at $X=\gamma$ is compared against, is in fact $T_{H''}(\gamma,\beta)$.
These two checks together ensure that \eqref{e:AggregationIdentity} holds at $X=\gamma$.
%Validity of the identity \eqref{e:AggregationIdentity} for two different $\lambda$ is proves that $q(X) = T_{\eta}(X,\beta)$ and $q'(X) &= T_{H'}(X,\beta)$.
For the two matching identities \eqref{e:MatchingIdentity1} and \eqref{e:MatchingIdentity2}, the values provided for $p(Y)$ and $p'(Y)$ at $Y=\beta$ are compared against the values of $q(X)$ and $q'(X)$ provided for $X=\alpha$ and $X=\alpha'$, respectively. 
These are exactly $T_\eta(\alpha,\beta)$ and  $T_{H'}(\alpha,\beta)$, assuming that $q(X)$ and $q'(X)$ satisfy \eqref{e:AggregationIdentity} for two different $\lambda$.
Again, the two checks together imply that \eqref{e:MatchingIdentity1} and \eqref{e:MatchingIdentity2} hold at the random challenge $Y=\beta$.




\subsubsection{Step 2. Extractor.}
Suppose that $\mathcal A$ is a probabilistic polynomial time adversary which succeeds the complete verifier with non-negligible probability $\varepsilon$ on given inputs $(\mathcal C$, $acc_\mathcal C'$, $acc_{dlog}'$, $x)$. 
%Considering this choice as part of the protocol's first message then 
As $K(\lambda)=m_1\cdot\ldots\cdot m_5$ is polynomial in $\lambda$, Lemma \ref{lem:ForkingLemma} guarantees a strict polynomial time algorithm $\mathcal T$ which calls $\mathcal A$ at most $2\cdot K(\lambda)/\varepsilon = \poly[\lambda]$ times and succeeds with a non-negligible probability of $\varepsilon/2$ in sampling an $(m_1,m_2,m_3,m_4,m_5)$-tree of accepting transcripts as needed for Step 1.
Each partial transcript $tr|_{i\leq 5}$ records the messages until and including the sampling of the last verifier challenge $\gamma$, before entering the batch evaluation protocol $\mathsf{Eval}$. 
By Lemma \ref{lem:ConditionalProbability} we may assume that for each of these partial transcripts $tr|_{i\leq 5}$, the probability that $\mathcal A$ succeeds on it is at least $\delta= \frac{\varepsilon^2}{8\cdot K(\lambda)} > \negl[\lambda]$.
By knowledge-soundness of the batch evaluation argument $\mathsf{Eval}$, there is a strict polynomial time extractor which calls $\mathcal A$ at most $\poly[\lambda]/\poly[\delta]$ times on each of the transcripts $tr|_{i\leq 5}$ %corresponding to the challenges $(\eta,\alpha,\beta,\lambda,\gamma)=(\eta_{i_1},\alpha_{i_2},\beta_{i_3},\lambda_{i_4},\gamma_{i_5})$
%, $(i_1,i_2,i_3,i_4,i_5)\in [1,m_1]\times\ldots\times [1,m_5]$ 
 and outputs the witness polynomials from $F^{<d+1}[X]$ (including commitment randomnesses) for 
\begin{itemize}
\item
$[\hat w(X)], [\hat z_A(X)], [\hat z_B(X)]$,
$[\hat U_1(X)]$, $[h_1(X)]$, $[T_{\vec\eta}(\alpha, X)]$,
\item
$[T_{H'}(\alpha',X)]$ from $acc_\mathcal C'$, as well as $[T_{\vec\eta}(X,\beta)]$, $[T_{H'}(X,\beta)]$, 
\item
$[T_{\vec\eta}(X,\beta)]+\lambda\cdot [T_{H'}(X,\beta)]$, and 
$[h(\vec\xi',X)]$ from $acc_{dlog}'$,
\end{itemize}
%for the $(\e)(\eta_{i_1},\alpha_{i_2},\beta_{i_3},\lambda_{i_4},\gamma_{i_5})$
of each of the $(\eta,\alpha,\beta,\lambda,\gamma)= (\eta_{i_1},\alpha_{i_2},\beta_{i_3},\lambda_{i_4},\gamma_{i_5})$ in the transcript tree.
These polynomials have values which pass all the verifier checks of the protocol.
Assuming the dlog commitment is computationally binding, the witness polynomials and commitment randomnesses for $[\hat w(X)]$, $[\hat z_A(X)]$, $[\hat z_B(X)]$, and the accumulator commitments $[T_{H'}(X,\beta)]$, $[h(\vec\xi',X)]$ overwhelmingly coincide for all the verifier challenges $(\eta_{i_1},\alpha_{i_2},\beta_{i_3},\lambda_{i_4},\gamma_{i_5})$, and for the same reason the polynomials for $[T_{\vec\eta} (\alpha,X)]$, $[\hat U_1(X)]$, $[h_1(X)]$, do not depend on $(\beta_{i_3},\lambda_{i_4},\gamma_{i_5})$.
Hence if we replace the commitments by these polynomials we obtain an $(m_1,m_2,m_3,m_4,m_5)$-tree of accepting transcripts as needed for Step 1 to conclude that  $y(X)=x(X)+(X^\ell-1)\cdot\hat w(X) \bmod (X^n-1)$ satisfies the R1CS identities for $\mathcal C_k$, and both $acc_T'$ and $acc_{dlog}'$ are correct. 
The overall run-time of the extractor is strictly bounded by $K(\lambda)\cdot \poly[\lambda]/\poly[\delta] = \poly[\lambda]/\poly[\varepsilon]$ and succeeds with a  non-negligible probability of at least $\varepsilon/4$.
By amplification we obtain the claimed extractor for knowledge-soundness.
% \[
%     \prob{
%         \begin{minipage}{2cm}
%             \centering
%             $tr$ is accepting
%         \end{minipage}
%         \:\left|\: 
%         \begin{minipage}{5.2cm}     
%         	$crs\leftarrow\setup(\lambda)$, 
% 	        \\
% 	        $(\mathcal C, acc_\mathcal C', acc_{dlog}',x)\leftarrow \mathcal A(crs)$,
% 	        \\
% 	        $tr\leftarrow\langle\mathcal A(),\verify(\mathcal C, acc_\mathcal C',acc_{dlog}',x)\rangle$
%         \end{minipage}
%         \right.
%     }> \negl[\lambda].
% \]


\subsection{Zero-knowledge}

Perfect honest verifier zero-knowledge of Protocol \ref{p:CompleteArgument} is an immediate consequence of perfect honest verifier zero-knowledge of the batch evaluation argument (Theorem \ref{thm:BatchEvaluation}) and the same property for the coboundary outer sumcheck.
The latter is obtained from the following auxiliary lemma. 
The proof of it is straightforward, and we leave it to the reader.
\begin{lem}
\label{l:zk}
Assume that $P$ follows Protocol \ref{p:CompleteArgument}.
Then the conditional distribution of $(v_1,v_2,v_3,v_4,v_5,v_6)=(\hat w(\beta), \hat y_A(\beta),\hat y_B(\beta), \hat U(g\cdot \beta), \hat U(\beta),h(\beta))$, conditional to $(\eta, \alpha, \beta)$, is uniform on the relation $\mathcal R_{\eta,\alpha,\beta}$ of all $(v_1,v_2,v_3,v_4, v_5,v_6)$ satisfying the outer sumcheck equation
\begin{multline*}
    T_\eta(\alpha,\beta)\cdot (x(\beta)+(\beta^\ell-1)\cdot v_1) - L_n(\beta,\alpha)\cdot (v_2 +\eta\cdot v_3 + \eta^2\cdot v_2\cdot v_3) 
    \\
    - v_4 + v_5 
    = v_6\cdot (\beta^n-1).
\end{multline*}
\end{lem}


%%%
%%% Lemma for coboundary sumcheck 
%%%
% \begin{lem}
% \label{l:zk}
% Assume that $P$ follows Protocol \ref{p:coboundarySumcheck}.
% Then the conditional distribution of $(v_1,v_2,v_3,v_4)=(\hat U(g\cdot z), \hat U(z), \hat p(z), h(z))$, conditional to $z\notin H$, is uniform on the relation $\mathcal R_z = \{(v_1,v_2,v_3,v_4)\in F^4: v_1 -v_2 - v_3 = v_4\cdot (z^n-1)\}$.
% \end{lem}

% \begin{proof}%[Proof of Lemma \ref{l:zk}]
% We assume that $z\notin H$ is fixed. 
% Since the coefficients $c_0$, $c_1$, $c_2$ for randomizing $p(X)$ and $U(X)$ are independent and uniformly drawn from $F$, the triple
% $\vec r =(c_1 + c_2\cdot g\cdot z, c_1 + c_2\cdot z , c_0)$ is uniformly distributed over $F^3$, and so is 
% \[
% (v_1, v_2, v_3) = (U(g\cdot z), U(z), p(z))+  (z^n-1)\cdot \vec r,
% \]
% as $z^n-1 \neq 0$. 
% As vector $(v_1,v_2,v_3,v_4)$ from an honest $P$ is always from $\mathcal R_z$, and $v_4$  is uniquely determined by $(v_1,v_2,v_3)$, the statement of the lemma is proven.
% \end{proof}

\medskip
Using Lemma \ref{l:zk} the simulator for the outer sumcheck is constructed as follows.
Given a consistent previous accumulator $acc'=(acc'_{\mathcal C}, acc'_{dlog})$ and any circuit input $x\in F^\ell$, it first samples $\eta$ and $(\alpha,\beta)$ uniformly from $F$ and $(F\setminus H)^2$, respectively, and $(v_1, v_2, v_3,v_4,v_5,v_6)$ uniformly from $\mathcal R_{\eta,\alpha,\beta}$ (by choosing $v_1,\ldots, v_5$ uniformly from $F$ and $v_6$ as determined by the outer sumcheck equation), and then crafts arbitrary polynomials $\hat w(X)$, $\hat y_A(X)$, $\hat y_B(X)$, $\hat U(X)$ and $h(X)$ of degree less than $d$ which evaluate at $X=\beta$ (and $\hat U(X)$ also at $X=g\cdot\beta$) to the corresponding values.
All these polynomials are committed using hiding randomnesses, and the aggregation rounds Step (3) and Step (4) of Protocol \ref{p:CompleteArgument} are performed as in an honest prover-verifier interaction. 
Since the dlog commitment scheme is perfectly hiding, the resulting conversation is identically distributed as in an ordinary honest prover-verifier interaction for Step (1-4).
These conversations are completed by calling the simulator for the batch evaluation argument on the collected commitments.
Since the latter partial transcripts are identically distributed to an honest prover-verifier interaction, so are the completed conversations. 


% \begin{thm} 
% Protocol \ref{p:coboundarySumcheck} is a computationally sound algebraic oracle proof for the sumcheck relation $\mathcal R=\{ (H\leq F^*, p(X)\in F^{\leq N(\lambda)}[X]) :  \sum_{x\in H} p(x) = 0 \}$ having soundness error $\varepsilon=\nicefrac{N(\lambda)}{|F|-|H|}$ and satisfying perfect honest verifier zero-knowledge.
% \end{thm}
% \begin{proof}
% \end{proof}

\section{Proof of Theorem \ref{thm:CoboundaryMarlin}}
\label{s:ProofCoboundaryMarlin}

The proof of Theorem \ref{thm:CoboundaryMarlin} is almost identical to that of Theorem \ref{thm:CompleteProtocol}, hence we only point out the differences. 
We assume that the indexer polynomial $row_M(X)$, $col_M(X)$, $row.col_M(X)$ and $val.row.col_M(X)$, $M=A,B,C$, as defined in Section \ref{s:Marlin} are verified against their commitments in a precomputation phase of the protocol.

\subsection{Knowledge-Soundness}
For knowledge-soundness, observe that Coboundary Marlin viewed as an algebraic oracle proof, is $(m_1$,$m_2$,$m_3$,$m_4)$-special sound with
\[
(m_1,m_2,m_3,m_4) = (3, n, 2\cdot d+1,4\cdot m - 3).
\]
Here, $d=\poly[\lambda]$ is the maximum degree of the oracle polynomials, $n$ and $m$ are the sizes of the domains $H$ and $K$, and $m_1,m_2,m_3,m_4$ correspond to the verifier challenges $\eta\in F$, $\alpha, \beta, \gamma \in F\setminus H$, respectively.
The reasoning for $m_1$, $m_2$, $m_3$ is as before, and $m_4=4\cdot m - 2$ by the degree of the inner sumcheck identity \eqref{e:InnerSumcheck}. 
Based on these modified soundness numbers, the strict polynomial time extractor is constructed as in Section \ref{s:ProofCompleteArgument} by using the forking lemma (Lemma \ref{lem:ForkingLemma}) and the extractor for the batch evaluation argument.

However, let us point out one subtly in regards of the inner sumcheck. 
Unlike the polynomial identities of the recursive argument, the inner sumcheck identity \ref{e:InnerSumcheck} as a multivariate polynomial in the oracles depends not just only on $\eta$, $\alpha$, $\beta$, but also on the oracle $[T_\eta(\alpha,X)]$ itself, as the response $v$ for $X=\beta$ is used for its definition. 
%\begin{multline*}
%\sum_{M=A,B,C} \eta_M \cdot val.row.col_M(X) \cdot b_{M}(X)
%\\
%= b(X) \cdot \left(\frac{v}{m} + U_2(g_K X) - U_2(X)\right) + h_2(X)\cdot (X^m -1 ),
%\end{multline*}
The value for $v$ is provided at the end of the oracle proof.
However, as the oracle is honest the provided values coincide for all completing transcripts corresponding to the same $(\eta_{i_1},\alpha_{i_2},\beta_{i_3})$.
Hence validating it at $m_4$ different challenges $\gamma_{i_4}$ proves that\footnotemark $v= T_{\eta}(\alpha,\beta)$.
\footnotetext{%
A similar argument holds for the modification pointed out in the footnote attached to the protocol description of the inner sumcheck. 
In this case the inner sumcheck identity is defined by $v= \frac{L_n(\beta,\alpha)\cdot \hat y_\eta(\beta) +  \hat U_1(g\cdot \beta) - \hat U_1(\beta) + h_1(\beta)\cdot (\beta^n-1)}{\hat y(\beta)}$, which depends on the challenges $\eta$, $\alpha$, $\beta$, and the oracles for $\hat w(X)$, $\hat y_A(X)$, $\hat y_B(X)$, $U_1(X)$, $h_1(X)$.
}
%where $b(X)$, $b_M(X)$ depend on $\alpha$, $\beta$, and $v$ depends on $\alpha$, $\beta$, and the oracle for $T_\eta(\alpha,X)$.


\subsection{Zero-knowledge}
The proof for perfect honest verifier zero-knowledge can be taken over almost verbatim, replacing completing of the simulated transcript for the outer sumcheck by an honest prover-verifier run of the inner sumcheck instead of the aggregation rounds.

\section{Polynomial commitment schemes}
\label{s:PolynomialCommitments}

% \begin{defn}[Hiding Commitment] 
% We say that a commitment scheme $(\setup,\comm)$ is \textit{perfectly hiding} if every polynomial adversary $\mathcal A$ has no advantage over pure guessing when distinguishing the commitments of two adversarily chosen messages,
% \begin{equation*}
%     \prob{
%     \:
%     b^\star = b 
%     \:\left|\: 
%     \begin{minipage}{4.5cm}
% 	    $pp\leftarrow\setup(\lambda)$, 
% 	    \\
% 	    for $i=1,2$
% 	        \\
% 	        \hspace*{0.5cm}$m_i\leftarrow\mathcal A(pp)$, 
% 	            $m_i\in\mathcal M_{pp}$, $i=1,2$
% 	        \\
% 	        \hspace*{0.5cm}$C_i\leftarrow \comm(m_i)$
% 	    \\
% 	    $b\sample\{0,1\}$,
% 	    $b^*\leftarrow\mathcal A(C_b,C_{1-b})$
%     \end{minipage}
% \right.
% } 
% = \frac{1}{2}.
% \end{equation*}
% \end{defn}

% \begin{defn}[Binding Commitment] 
% A commitment scheme $(\setup,\comm)$ is \textit{computationally binding} if for every p.p.t. adversary $\mathcal A$,
% the probability to find two different message-randomness combinations with the same commitment is negligible:
% \begin{equation*}
%     \prob{
%     \left.
%         \begin{minipage}{4.3cm}
%             \centering
%             $\comm(m_1;r_1) = \comm(m_2;r_2)$
%             \\
%             $\wedge$
%             \\
%             $(m_1,r_1)\neq(m_2,r_2)$
%         \end{minipage}
%     \:
%     \right|
%     \: 
%     \begin{minipage}{4.1cm}
% 	    $pp\leftarrow\setup(\lambda)$, 
%         \\
% 	    $(m_1,r_1,m_2,r_2)\leftarrow\mathcal A(pp)$,
% 	    \\
% 	    $(m_i,r_i)\in M_{pp}\times R_{pp}$, $i=1,2$
%     \end{minipage}
%     \:
% } 
% = \negl[\lambda].
% \end{equation*}
% \end{defn}

% A commitment scheme in which the messages are polynomials over a finite field is a \textit{polynomial commitment scheme}, if it comes with further two interactive p.p.t. algorithms $\open$, $\verify$ such that $(\setup,\open,\verify)$ is an argument system for the relation
% \[
% \mathcal R = \big\{
%     ((C,x,v),(p(X),r)) ~:~ C=\comm(p(X);r) \wedge p(x) = v
% \big\}.
% \]
% % Formally, a polynomial commitment scheme consists of four p.p.t. algorithms 
% % \[
% % (\setup, \comm, \open, \verify).
% % \]
% The system parameters $pp$ for the commitment scheme $(\setup,\comm)$ consist of a committer key $ck$ and a verifier key $vk$ supporting polynomials over $F=F_{pp}$ of degree at most $N$, where $N$ is polynomial in $\lambda$.

We regard a polynomial commitment scheme consisting of four probabilistic polynomial time algorithms 
\[
(\setup, \comm, \prove, \verify).
\]
Given the security parameter $\lambda$, $(ck,vk)\leftarrow\setup(\lambda)$ generates a common reference string consisting of a committer key $ck$ and a verifier key $vk$ supporting polynomials over some finite field $F$ having degree of at most $N=N(\lambda)$, where $N$ is polynomial in $\lambda$. 
Given the committer key $ck$, the commitment of a polynomial $p(X)$ of degree at most $N$ is computed by $C=\comm(p(X); r)$, where $r$ denotes the used random coins.
(We again omit $ck$ from the inputs for brevity.)
We regard $(\setup,\prove,\verify)$ as a succinct interactive argument system for the relation
\[
\mathcal R = \big\{
    ((C,x,v),(p(X),r)) ~:~ C=\comm(p(X);r) \wedge p(x) = v
\big\},
\]
and call the polynomial commitment scheme to satisfy completeness, zero-knowledge and witness-extended emulation if the interactive argument system does.
We refer to the interaction $\langle \open,\verify\rangle$ as opening proof for the polynomial $p(X)$ at the point $x\in F$. 

The security notions \textit{computational binding} and \textit{perfect hiding} are as for general non-interactive commitment schemes $(\setup,\comm)$. 
For the sake of brevity, we directly cite them applied to polynomial commitment schemes:

\begin{defn}[Perfect Hiding] 
\label{def:Hiding}
We say that a polynomial commitment scheme $(\setup,\comm,$ $\open,\verify)$ is \textit{perfectly hiding} if every polynomial adversary $\mathcal A$ has no advantage over pure guessing when distinguishing the commitments of two adversarially chosen polynomials,
\begin{equation*}
    \prob{
    \:
    b^\star = b 
    \:\left|\: 
    \begin{minipage}{6.8cm}
	    $(ck,vk)\leftarrow\setup(\lambda)$, 
	    \\
	    for $i=1,2$
	        \\
	        \hspace*{0.5cm}$p_i(X)\leftarrow\mathcal A(ck,vk)$, 
            $\deg (p_i(X))\leq N(\lambda)$,
	        \\
	        \hspace*{0.5cm}$C_i\leftarrow \comm(p_i(X))$
	    \\
	    $b\sample\{0,1\}$,
	    $b^*\leftarrow\mathcal A(C_b,C_{1-b})$
    \end{minipage}
\right.
} 
= \frac{1}{2}.
\end{equation*}
\end{defn}

\begin{defn}[Computational Binding] 
\label{def:Binding}
A polynomial commitment scheme $(\setup,\comm,$ $\open,\verify)$ is \textit{computationally binding} if for every p.p.t. adversary $\mathcal A$,
the probability to find two different messages $(p_i(X),r_i)$, $i=1,2$, having the same commitment is negligible:
\begin{multline*}
    \prob{
    \left.
        \begin{minipage}{5.3cm}
            \centering
            $\comm(p_1(X);r_1) = \comm(p_2(X);r_2)$
            \\
            $\wedge$
            \\
            $(p_1(X),r_1)\neq(p_2(X),r_2)$
        \end{minipage}
    \right| 
    \begin{minipage}{5.3cm}
	    $(ck,vk)\leftarrow\setup(\lambda)$, 
        \\
	    $(r_1,p_1(X),r_2,p_2(X))\leftarrow\mathcal A(ck,vk)$ 
    \end{minipage}
} 
\\
= \negl[\lambda].
\end{multline*}
\end{defn}
%\todo{?better hiding and binding for (normal) commmitment schemes?}

We further make use the notion of a homomorphic schemes, again directly applied to polynomial commitment schemes:
\begin{defn}[Homomorphic commitment]
A polynomial commitment scheme $(\setup$, $\comm,\open,\verify)$ with commitments in a \textit{commitment group} $(\mathcal G,+)$ is \textit{homomorphic} (or, \textit{linear}), if
\begin{equation*}
    \comm(p_1(X);r_1) + \comm(p_2(X);r_2) = \comm(p_1(X)+p_2(X);r_1+r_2).
\end{equation*}
\end{defn}


\section{The dlog commitment scheme from \cite{Buenz}}

%As the inner product argument based commitment schemes \cite{BootleGroth}, \cite{Bulletproofs}, \cite{Wahby}, 
The dlog polynomial commitment scheme from \cite{Buenz} is an ordinary Pedersen vector commitment. 
Given the coefficient vector $\vec c = (c_i)_{i=0}^{d-1}$ of a polynomial $p(X)$ from $F[X]$ of degree at most $d$, its commitment is the Pedersen linear combination
\begin{align*}
\comm(p(X);r) = r\cdot S + c_0\cdot G_0 + \ldots + c_{d-1}\cdot G_{d-1},
\end{align*}
where $S$ and $(G_i)$ is the committer key, and the optional hiding randomness $r$ is uniformly drawn from $F$.
As in \cite{BootleGroth}, \cite{Bulletproofs}, or \cite{Wahby}, 
the opening proof is an inner product argument, which uses $k=\log(d)$ rounds to gradually reduce the size polynomial in question by one half until ending up with a single-coefficient instance. 
%Assuming that $d$ is a power of two, 
In the course of the reduction the committer key is repeatedly folded into a final committer key $G_f$ for the single-coefficient claim, which depends linearly on the initial committer key by
\[
G_f = h_0\cdot G_0 + \ldots + h_{d-1}\cdot G_{d-1},
\]
where $(h_i)$ are the coefficients of the \textit{reduction polynomial} 
\begin{equation*}
%\label{e:BulletPolynomial}
h(\vec\xi, X) = \prod_{i=0}^{k-1} ( 1- \xi_{k-1-i} \cdot X^{2^i}),
\end{equation*}
where $\vec\xi=(\xi_i)_{i=0}^{k-1}$ are the random challenges of the reduction steps.
See \cite{Buenz} for a detailed description.
% The witness polynomial $p(X)$ with a randomly sampled mask polynomial $s(X)$ of degree $N-1$ in the first step of the protocol.
% The prover computes a hiding commitment $C' = \comm(s(X);r')$ and 
% reduces the evaluation claim on $p(X)$ to that of the linear combination $\hat p(X)= s(X)+\lambda\cdot p(X)$, where $\lambda\sample F_q$ is sampled by the verifier.

% For a sake of completeness we give an informal description of the dlog polynomial commitment scheme from \cite{Buenz}, which is a variant of the inner product argument from \cite{BootleGroth}, \cite{Bulletproofs}, \cite{Wahby}.

% Let $G$ be a group of prime order $p$. 
% Having elliptic curves in mind, we use additive notation for the group operation and denote by $F$ the prime field of scalars.
% The committer key consists of $N=N(\lambda)$ plus another two random elements, $ck=\{H, S, \vec G= (G_i)_{i=0}^{N-1}\}$.
% %The vector $\vec G$ is used for committing a polynomial $F$, where $S$ is used for blinding, and $H$ is for the opening argument.
% % all of which are elements from a prime order group $G$.
% Given the coefficient vector $\vec c = (c_i)_{i=0}^{N-1}$ of a polynomial $p(X)$ from $F[X]$ of degree at most $N$, its commitment is the Pedersen linear combination
% \begin{align*}
% \comm(p(X);r) &= r\cdot S + \langle \vec c, \vec G\rangle 
% \\
% &= r\cdot S + c_0\cdot G_0 + \ldots + c_{N-1}\cdot G_{N-1},
% \end{align*}
% where $r$ is drawn uniformly from $F$.

% The opening argument proves that the inner product $\langle \vec c,\vec b\rangle$ of $\vec c$ with the power vector $\vec b = (1,x,x^2,\ldots, x^{N-1})$ equals the claimed value of $p(X)$ at the queried point $x$, and uses the folding technique from \cite{BootleGroth}.
% We explain the non-hiding case first.
% In a first step the inner product claim is transformed to the claim that the preimage of 
% \[
% C = \langle\vec c,\vec b\rangle\cdot H' + \langle \vec c, \vec G \rangle,  %+ \langle \vec b, \vec G\rangle
% \]
% where $H' = \alpha\cdot H$ with $\alpha\sample F$, encodes the vector and its inner product with $\vec b$.
% %having witnesses  $\vec c=(c_0,\ldots, c_{N-1})$, $\vec b= (1,x,x^2,\ldots, x^{N-1})$ with inner product $(\vec c,\vec b)$, 
% Starting with $C$ any such instance is replaced by an instance 
% \begin{align*}
% C' 
% &= \xi^{-1}\cdot L + C + \xi\cdot R
% \\
% &= \langle\vec c',\vec b'\rangle\cdot H' + \langle \vec c', \vec G' \rangle % + \langle \vec b', \vec G'\rangle 
% \end{align*}
% of half the size, belonging to the modified witness vectors
% \begin{align*}
%     \vec c'&= \mathsf{l}(\vec c) + \xi^{-1}\cdot\mathsf{r}(\vec c),
%     \\
%     \vec b' &= \mathsf{l}(\vec b) + \xi\cdot\mathsf{r}(\vec b),
% \end{align*}
% and base points $\vec G'= \mathsf{l}(\vec G) + \xi\cdot\mathsf{r}(\vec G)$, where $\mathsf{l}(.)$ and $\mathsf{r}(.)$ denotes the left and right half of a vector, respectively.
% Here  $\xi\sample F^*$ is sampled by the verifier, and
% \begin{align*}
%     L &= (\mathsf{r}(\vec c), \mathsf{l}(\vec b))\cdot H' + \langle \mathsf{r}(c), \mathsf{l}(\vec G)\rangle,
%     \\
%     R &= (\mathsf{l}(\vec c), \mathsf{r}(\vec b))\cdot H' + \langle \mathsf{l}(c), \mathsf{r}(\vec G)\rangle,
% \end{align*}
% are the `correction terms' sent by the prover to the verifier.
% This random reduction is repeated $n=\log_2(N)$ times until one arrives at the final single-coefficient instance 
% \[
% C_{f} = (c_{f}\cdot b_f)\cdot H' +  c_f\cdot G_f %+ b_f\cdot G_f.
% \]
% (We assume that $N$ is always a power of two.)
% This instance is proven by providing $c_f$ in plain to the verifier, who checks the equation with $G_f$ and $b_f$ computed by herself.
%
% % To speed up the verifier in computing $G_f$, which is the result of the folding procedure on $\vec G$ using the reduction challenges $(\xi_i)_{i=0}^{N-1}$, one uses the observation that both $b_f$ and $G_f$ depend linearly on the power vector of $x$ and $\vec G$, respectively.
% % Hence
% % \begin{equation*}
% % b_f = h(\vec\xi, x),
% % \quad
% % G_f = \langle h(\vec\xi,X), \vec G\rangle,
% % \end{equation*}
% % for some polynomial $h(\vec\xi,X)$ of degree $N-1$, which has the explicit form
% % \begin{equation*}
% % h(\vec\xi, X) = \prod_{i=0}^{n-1} ( 1- \xi_{n-1-i} \cdot X^{2^i}).
% % \end{equation*}

% For obtaining zero-knowledge \cite{Buenz} randomize the witness polynomial $p(X)$ with a randomly sampled mask polynomial $s(X)$ of degree $N-1$ in the first step of the protocol.
% The prover computes a hiding commitment $C' = \comm(s(X);r')$ and 
% reduces the evaluation claim on $p(X)$ to that of the linear combination $\hat p(X)= s(X)+\lambda\cdot p(X)$, where $\lambda\sample F_q$ is sampled by the verifier.


\begin{thm}[\cite{BootleGroth, Buenz}]
\label{thm:dlog}
Under the assumption that the dlog commitment scheme is computationally binding, the  opening argument
%$(\setup_{dlog}, \open_{dlog},\verify_{dlog})$ 
is an honest verifier zero-knowledge (Definition \ref{def:ZeroKnowledge}) argument of knowledge (Definition \ref{def:ArgumentOfKnowledge}).
\end{thm}

Zero-knowledge is proven in \cite{Buenz}.
To obtain a strict polynomial time extractor as demanded by Definition \ref{def:ArgumentOfKnowledge}, one proceeds as in \cite{BootleGroth} to construct an expected polynomial time extractor with average runtime $\mu$ inverse proportional to the success probability $\varepsilon$ of the adversary. 
Truncating it to strict polynomial time $2\cdot \mu/\varepsilon$ zields a running time bounded by $\poly[\lambda]/\poly[\varepsilon]$ calls of $\mathcal A$ and still maintains a success probability of $\varepsilon/2$.   

%Theorem \ref{thm:dlog} extends to the batch evaluation protocol from \cite{HaloInfinite}, as discussed in the following section.
%(However, they prove knowledge soundness and not witness-extended extractability.)

\section{The batch evaluation protocol from \cite{HaloInfinite}}
\label{s:MultiPointSinglePoint}

We give an informal description of the protocol from \cite{HaloInfinite}, Section $5.2$, for proving a multi-point opening claim
\[
p_i(x_i)=y_i, \quad i=1,\ldots,m,
\]
of given polynomials $p_i(X)$, $i=1,\ldots,m$, for a linear commitment scheme $(\setup, \comm, \open,\verify)$.
The multi-point evaluation claim is equivalent to the following system of algebraic identities over the set $\Omega=\{x_1,\ldots, x_m\}$, 
\[
p_i(X)\cdot L_{m}(x_i,X) - y_i = 0 \mod z(X),\quad i=1,\ldots,m,
\]
where $L_{m}(x_i,X)$ is the Lagrange polynomial for $\Omega$, and $z(X)=\prod_{x\in\Omega} (X-x)$ is the vanishing polynomial of $\Omega$.
A more practical system of identities is achieved if we replace the Lagrange polynomials by their non-normalized variant 
\[
z_i(X) = \frac{z(X)}{X-x_i}= \prod_{x\in\Omega\setminus\{x_i\}}(X-x).
\]
These $m$ identities are reduced to a single identity over $\Omega$ in the usual manner:
The prover receives a random scalar $\rho\sample F$ from the verifier, and shows the combined identity
\[
\sum_{i=1}^m \rho^{i-1} \cdot \left( p_i(X) - y_i\right) \cdot z_i(X)
    = 0 \mod z(X)
\]
instead.
%If the $p_i(X)$ satisfy the combined identity, then they overwhelmingly satisfy the initial evaluation claim.
For this, the verifier provides an oracle for the quotient polynomial $q(X)$ in the non-modular identity
\[
\sum_{i=1}^m \rho^{i-1} \cdot \left( p_i(X) - y_i\right)\cdot z_i(X) = q(X)\cdot z(X),
\]
which is probed at a fresh random point $x\sample F$.
This identity is verified by showing that the linear combination $\sum_{i=1}^m \rho^{i-1} \cdot z_i(x) \cdot  p_i(X) -  z(x)\cdot q(X)$ opens at $x$ to the expected value $v = \sum_{i=1}^{m} \rho^{i-1}\cdot y_i\cdot z_i(x)$.

\begin{thm}[\cite{HaloInfinite}, Theorem 6]
\label{thm:BatchEvaluation}
Assume that the commitment scheme is computationally binding.
If the opening argument for the polynomial commitment scheme is a perfect honest verifier zero-knowledge argument of knowledge (Definition \ref{def:ArgumentOfKnowledge}), then the same holds for the batch evaluation protocol.
%If the opening argument is honest verifier zero-knowledge, the same holds for the batch evaluation protocol.
\end{thm}



% We note that despite \cite{HaloInfinite} use knowledge-soundness for defining arguments of knowledge, their proof easily extends to witness-extended emulation.
% The rewinding extractor samples $m$ different $\rho$'s and for each of these two different $x$'s with accepting transcripts, from which it is able to derive the witness polynomials $p_i(X)$ together with their hiding randomnesses from the extracted witnesses for the linear combinations.
% It comes at no extra cost to provide also the transcript of one of the runs, which is indistinguishable from a normal interaction.

% This is not exactly how they approach it, but maybe also doable:
% The rewinding extractor uses the fact that the $z_i(X)$ together with $z(X)$ form a set of $m+1$ linear independent polynomials of low degree.
% Given $\rho\neq 0$ and the commitment for $q(X)$ it samples $m+1$ different $x$'s so that it is able to uniquely derive the witness polynomials $p_i(X)$ and $q(X)$ together with their hiding randomnesses from the extracted witnesses for the linear combinations.
% Sampling $m$ different $\rho$'s, the subsequent extracted $p_i(X)$ are all the same (unless we break the binding property of the commitment scheme) and 



\section{Segmentation of linear commitment schemes}
\label{s:Segmentation}

\textit{Segmentation} of a homomorphic polynomial commitment scheme is a useful technique to improve the computational effort of the prover at the cost of increasing the commitment size\footnotemark.
\footnotetext{%
We learned this technique from \cite{Mina} but believe that it is commonly known.
}
One chooses an undersized committer key $ck=(G_0,\ldots,G_{s-1})$, where the \textit{segment size} $s$ is typically magnitudes smaller than the targeted maximum degree $d$, and extends its domain beyond degree $s-1$ by decomposing a polynomial $p(X)$ into
\[
p(X) = p_0(X) + X^s\cdot p_1(X) + \ldots + X^{(k-1)\cdot s} \cdot p_k(X), 
\]
with each $p_i(X)$, $i=1,\ldots, k$, of degree at most $s$. 
(If $d$ is the degree of $p(X)$ then the number $k$ of segment polynomials is equal to $\ceil{(d+1)/s}$.) 
The commitment of $p(X)$ is then defined as the vector of the commitments of its segment polynomials using $ck$, 
\[
\comm(p(X)) := \left(\comm(p_0(X)),\ldots, \comm(p_k(X))\right).
\]
Every evaluation claim $p(x)=v$ of the full-size polynomial is translated to the same claim on the linear combination of its segment polynomials,
\[
LC_x(p_0(X),\ldots,p_k(X)) = 
p_0(X) + x^s\cdot p_1(X) + \ldots + x^{(k-1)\cdot s} \cdot p_k(X),
\]
which is efficiently proven by leveraging the homomorphic property of the scheme.

% For the dlog prover, the most expensive operation is the stepwise folding of the committer key in the course of the reduction steps of the inner product argument. 
% The number of scalar multiplications needed for that is quadratic in the number of reduction steps. 
% Hence reducing the committer by a factor $c$ reduces the folding costs by a factor of $c^2$.
% However we increase the commitment sizes by $c$, and in the same way the batching/combining effort of the verifier. 
% This increased verifier effort is not essential in primitive, but in recursion this may lead to a significant increase of the number of constraints if one does not balance $k$ the maximum number of segments carefully.



\section{Facts on the Lagrange kernel}
\label{s:LagrangeKernel}
Let $H=\{x: x^n-1=0\}$ be an order $n$ subgroup of the multiplicative group of a finite field $F$.
The Lagrange kernel 
\[
L_n(X,Y) =\frac{1}{n}\cdot \left( 1+ \sum_{i=1}^{n-1} X^i\cdot Y^{n-i}\right) 
\]
is the unique bivariate symmetric polynomial of individual degree at most $n-1$, such that for $y\in H$ the function $L(X,y)$ restricted to $H$ equals the Lagrange function $L_y(X)$, which evaluates to one at $X=y$, and zero otherwise.
%such that restricted to $H\times H$ we have $L(X,Y) = 1$ if $X=Y$ and $L(X,Y)=0$ if $X\neq Y$.
The  kernel has the succinct representation
\[
L_n(X, Y)=\frac{1}{n}\cdot \frac{Y\cdot Z_H(X) - X\cdot Z_H(Y)}{ X- Y},
\]
where $Z_H(X)=X^n-1$ is the vanishing polynomial of $H$.
Lagrange kernels represent point evaluation, as characterized by the following simple Lemma.

\begin{lem}
\label{lem:LagrangeKernel}
Suppose that $H\subset F^*$ is a multiplicative subgroup of order $n$ and $p(X)$ is a polynomial of degree $deg(p(X))\leq n-1$. Then for every $z$ in $F$,
\[
\big\langle L_n(X, z), p(X)\big\rangle_H = \sum_{x\in H} L_n(z, x) \cdot p(x) = p(z).
\] 
\end{lem}
\begin{proof}
Since $p(X) = \sum_{y\in H} p(y)\cdot L_n(X,y)$, it suffices to show the claim for $p(X)=L_n(X,y)$, with $y\in H$.
By the property of $L_n(X,y)$, we have $\big\langle L_n(X, z)$, $L_n(X,y) \big\rangle_H =L_n(y,z)$, which by symmetry is equal to $L_n(X,y)$ at $X=z$.
This completes the proof of the Lemma.
\end{proof}

Marlin \cite{Marlin} uses the generalized derivative 
\[
u_H(X,Y)= \frac{Z_H(X)-Z_H(Y)}{X-Y}
\]
instead of $L_n(X,Y)$ (again as element from $F[X,Y]/\langle X^n-1,Y^n-1\rangle$).
For the generalized derivative a similar inner product formula holds:
For every $z\in F$,
\begin{align*}
\big\langle u_H(X, z), p(X)\big\rangle_H &=  \big\langle u_H(X,X)\cdot L_H(X, z), p(X)\big\rangle_H
\\
&= \big\langle L_n(X, z), u_H(X,X)\cdot p(X)\big\rangle_H = p^*(z),
\end{align*}
where $p^*(X) = u_H(X,X)\cdot p(X) \bmod (X^n-1)$.

\end{document}